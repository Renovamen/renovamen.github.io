import{_ as r,K as a,o as n,m as t,L as o}from"./app-0916aa90.js";const i={class:"prose prose-lg m-auto text-left"},l=o('<p><div class="table-of-contents"><ul><li><a href="#online-courses">Online courses </a></li><li><a href="#libraries-and-open-source">Libraries and open source </a></li><li><a href="#active-blogs">Active blogs </a></li><li><a href="#books">Books </a></li><li><a href="#miscellaneous">Miscellaneous </a></li><li><a href="#diy-projects-and-data-sets">DIY projects and data sets </a></li><li><a href="#nlp-on-social-media">NLP on social media </a></li></ul></div></p><p>来源：<a href="https://towardsdatascience.com/how-to-get-started-in-nlp-6a62aa4eaeff" target="_blank" rel="noopener noreferrer">Melanie Tosik（Twitter:@meltomene）列出的 NLP 学习资源清单</a></p><h2 id="online-courses" tabindex="-1">Online courses <a class="header-anchor" href="#online-courses" aria-hidden="true">#</a></h2><ul><li><p><a href="https://www.youtube.com/playlist?list=PL8FFE3F391203C98C" target="_blank" rel="noopener noreferrer">Dan Jurafsky &amp; Chris Manning: Natural Language Processing</a></p></li><li><p><a href="http://cs224d.stanford.edu/syllabus.html" target="_blank" rel="noopener noreferrer">Stanford CS224d: Deep Learning for Natural Language Processing</a> [更高级的机器学习算法、深度学习和 NLP 的神经网络架构]</p></li><li><p><a href="https://www.youtube.com/playlist?list=PLLssT5z_DsK8BdawOVCCaTCO99Ya58ryR" target="_blank" rel="noopener noreferrer">Coursera: Introduction to Natural Language Processing</a> [密西根大学的 NLP 课程]</p></li></ul><h2 id="libraries-and-open-source" tabindex="-1">Libraries and open source <a class="header-anchor" href="#libraries-and-open-source" aria-hidden="true">#</a></h2><ul><li><p><strong>spaCy</strong> (<a href="https://spacy.io" target="_blank" rel="noopener noreferrer">website</a>, <a href="https://explosion.ai/blog/" target="_blank" rel="noopener noreferrer">blog</a>) [Python；新兴的开放源码库并自带<a href="https://spacy.io/usage/spacy-101" target="_blank" rel="noopener noreferrer">炫酷的用法示例</a>、API 文档和<a href="https://spacy.io/docs/usage/showcase" target="_blank" rel="noopener noreferrer">演示应用程序</a>]</p></li><li><p><strong>Natural Language Toolkit (NLTK)</strong> (<a href="http://www.nltk.org/" target="_blank" rel="noopener noreferrer">website</a>, <a href="http://www.nltk.org/book/" target="_blank" rel="noopener noreferrer">book</a>) [Python；NLP 实用编程介绍，主要用于教学目的]</p></li><li><p><strong>Stanford CoreNLP</strong> (<a href="https://stanfordnlp.github.io/CoreNLP/" target="_blank" rel="noopener noreferrer">website</a>) [由 Java 开发的高质量的自然语言分析工具包]</p></li><li><p><strong>AllenNLP</strong> (<a href="https://allennlp.org/" target="_blank" rel="noopener noreferrer">website</a>) [Python；基于 PyTorch 的 NLP 研究库]</p></li><li><p><strong>fastText</strong> (<a href="https://fasttext.cc/" target="_blank" rel="noopener noreferrer">website</a>) [C++；高效的文本分类（text classification）和表示学习（representation learning）工具]</p></li></ul><h2 id="active-blogs" tabindex="-1">Active blogs <a class="header-anchor" href="#active-blogs" aria-hidden="true">#</a></h2><ul><li><p><a href="https://nlpers.blogspot.com/natural" target="_blank" rel="noopener noreferrer">language processing blog</a> （Hal Daumé III）</p></li><li><p><a href="http://languagelog.ldc.upenn.edu/nll/" target="_blank" rel="noopener noreferrer">Language Log</a> （Mark Liberman）</p></li><li><p><a href="https://research.googleblog.com/" target="_blank" rel="noopener noreferrer">Google Research blog</a></p></li><li><p><a href="https://explosion.ai/blog/" target="_blank" rel="noopener noreferrer">Explosion AI blog</a></p></li><li><p><a href="https://medium.com/huggingface" target="_blank" rel="noopener noreferrer">Hugging Face</a></p></li><li><p><a href="http://ruder.io/#open" target="_blank" rel="noopener noreferrer">Sebastian Ruder’s blog</a></p></li></ul><h2 id="books" tabindex="-1">Books <a class="header-anchor" href="#books" aria-hidden="true">#</a></h2><ul><li><p><a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="noopener noreferrer">Speech and Language Processing</a> （Jurafsky and Martin）[经典的 NLP 教科书，涵盖了所有 NLP 的基础知识，第 3 版即将出版]</p></li><li><p><a href="https://nlp.stanford.edu/fsnlp/" target="_blank" rel="noopener noreferrer">Foundations of Statistical Natural Language Processing</a> （Manning and Schütze）[更高级的统计 NLP 方法]</p></li><li><p><a href="https://nlp.stanford.edu/IR-book/" target="_blank" rel="noopener noreferrer">Introduction to Information Retrieval</a> （Manning, Raghavan and Schütze）[关于排名/搜索的优秀参考书]</p></li><li><p><a href="https://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037" target="_blank" rel="noopener noreferrer">Neural Network Methods in Natural Language Processing</a> （Goldberg）[深入介绍 NLP 的 NN 方法，和相对应的入门书籍]</p></li><li><p><a href="http://www.morganclaypool.com/doi/abs/10.2200/S00493ED1V01Y201303HLT020" target="_blank" rel="noopener noreferrer">Linguistic Fundamentals for Natural Language Processing</a> （Bender）[更成功的 NLP 的词法和句法]</p></li><li><p><a href="http://www.deeplearningbook.org/" target="_blank" rel="noopener noreferrer">Deep Learning</a> （Goodfellow, Courville and Bengio）[很好的深度学习介绍]</p></li></ul><h2 id="miscellaneous" tabindex="-1">Miscellaneous <a class="header-anchor" href="#miscellaneous" aria-hidden="true">#</a></h2><ul><li><p><a href="https://www.tensorflow.org/versions/master/tutorials/word2vec/index.html" target="_blank" rel="noopener noreferrer">How to build a word2vec model in TensorFlow</a> [学习指南]</p></li><li><p><a href="https://github.com/andrewt3000/dl4nlp" target="_blank" rel="noopener noreferrer">Deep Learning for NLP resources</a> [按主题分类的关于深度学习的顶尖资源的概述]</p></li><li><p><a href="http://mitp.nautil.us/article/170/last-words-computational-linguistics-and-deep-learning" target="_blank" rel="noopener noreferrer">Last Words: Computational Linguistics and Deep Learning — A look at the importance of Natural Language Processing.</a> （Manning）[文章]</p></li><li><p><a href="https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf" target="_blank" rel="noopener noreferrer">Natural Language Understanding with Distributed Representation</a> （Cho）[关于 NLU 的 ML / NN 方法的独立讲义]</p></li><li><p><a href="http://www.isi.edu/natural-language/people/bayes-with-tears.pdf" target="_blank" rel="noopener noreferrer">Bayesian Inference with Tears</a> （Knight）[教程工作簿]</p></li><li><p><a href="http://aclanthology.info/" target="_blank" rel="noopener noreferrer">Association for Computational Linguistics</a> （ACL）[期刊选集]</p></li><li><p><a href="https://www.quora.com/How-do-I-learn-Natural-Language-Processing" target="_blank" rel="noopener noreferrer">Quora: How do I learn Natural Language Processing?</a></p></li><li><p><a href="https://docs.google.com/document/d/1mkB6KA7KuzNeoc9jW3mfOthv_6Uberxs8l2H7BmJdzg/edit" target="_blank" rel="noopener noreferrer">Natural Language Understanding and Computational Semantics</a> （Bowman）[开源的课程大纲和完整幻灯片]</p></li><li><p><a href="http://www.fast.ai/" target="_blank" rel="noopener noreferrer">fast.ai</a> [“Making neural nets uncool again”]</p></li></ul><h2 id="diy-projects-and-data-sets" tabindex="-1">DIY projects and data sets <a class="header-anchor" href="#diy-projects-and-data-sets" aria-hidden="true">#</a></h2><p>Nicolas Iderhoff 已经创建了一份<a href="https://github.com/niderhoff/nlp-datasets" target="_blank" rel="noopener noreferrer">公开、详尽的 NLP 数据集的列表</a>。除了这些，这里还有一些推荐的项目：</p><ul><li><p>Implement a <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging" target="_blank" rel="noopener noreferrer">part-of-speech (POS) tagger (词性标注)</a> based on a <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model" target="_blank" rel="noopener noreferrer">hidden Markov model (HMM) (隐马尔可夫模型)</a></p></li><li><p>Implement the <a href="https://en.wikipedia.org/wiki/CYK_algorithm" target="_blank" rel="noopener noreferrer">CYK algorithm</a> for parsing <a href="https://en.wikipedia.org/wiki/Context-free_grammar" target="_blank" rel="noopener noreferrer">context-free grammars</a></p></li><li><p>Implement <a href="https://en.wikipedia.org/wiki/Semantic_similarity" target="_blank" rel="noopener noreferrer">semantic similarity (语义相似度)</a> between two given words in a collection of text, e.g. <a href="https://en.wikipedia.org/wiki/Pointwise_mutual_information" target="_blank" rel="noopener noreferrer">pointwise mutual information (PMI) (点互信息)</a></p></li><li><p>Implement a <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" target="_blank" rel="noopener noreferrer">Naive Bayes classifier (朴素贝叶斯分类器)</a> to <a href="https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering" target="_blank" rel="noopener noreferrer">filter spam</a></p></li><li><p>Implement a <a href="https://en.wikipedia.org/wiki/Spell_checker" target="_blank" rel="noopener noreferrer">spell checker</a> based on <a href="https://en.wikipedia.org/wiki/Edit_distance" target="_blank" rel="noopener noreferrer">edit distances</a> between words</p></li><li><p>Implement a <a href="https://en.wikipedia.org/wiki/Markov_chain" target="_blank" rel="noopener noreferrer">Markov chain (马尔科夫链)</a> text generator</p></li><li><p>Implement a <a href="https://en.wikipedia.org/wiki/Topic_model" target="_blank" rel="noopener noreferrer">topic model</a> using <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" target="_blank" rel="noopener noreferrer">latent Dirichlet allocation (LDA)</a></p></li><li><p>Use <a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="noopener noreferrer">word2vec</a> to generate word embeddings from a large text corpus, e.g. <a href="https://en.wikipedia.org/wiki/Wikipedia:Database_download" target="_blank" rel="noopener noreferrer">Wikipedia</a></p></li><li><p>Use <a href="https://en.wikipedia.org/wiki/K-means_clustering" target="_blank" rel="noopener noreferrer">k-means</a> to cluster <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" target="_blank" rel="noopener noreferrer">tf-idf</a> vectors of text, e.g. news articles</p></li><li><p>Implement a <a href="https://en.wikipedia.org/wiki/Named-entity_recognition" target="_blank" rel="noopener noreferrer">named-entity recognizer (NER) (命名实体识别)</a> (also called a name tagger), e.g. following the <a href="https://www.clips.uantwerpen.be/conll2003/ner/" target="_blank" rel="noopener noreferrer">CoNLL-2003 shared task</a></p></li></ul><h2 id="nlp-on-social-media" tabindex="-1">NLP on social media <a class="header-anchor" href="#nlp-on-social-media" aria-hidden="true">#</a></h2><ul><li><p>Twitter: <a href="https://twitter.com/hashtag/nlproc" target="_blank" rel="noopener noreferrer">#nlproc</a>, <a href="https://twitter.com/hashtag/nlproc" target="_blank" rel="noopener noreferrer">list of NLPers</a> (by Jason Baldrige)</p></li><li><p>Reddit: <a href="https://www.reddit.com/r/LanguageTechnology" target="_blank" rel="noopener noreferrer">/r/LanguageTechnology</a></p></li><li><p>Medium: <a href="https://medium.com/tag/nlp" target="_blank" rel="noopener noreferrer">NLP</a></p></li></ul>',17),s=[l],b="NLP 不入门直接放弃",k=["nlp"],w=[{property:"og:title",content:"NLP 不入门直接放弃"}],p={__name:"2018-02-17-nlp-resource",setup(g,{expose:e}){return e({frontmatter:{title:"NLP 不入门直接放弃",tags:["nlp"],meta:[{property:"og:title",content:"NLP 不入门直接放弃"}]}}),a({title:"NLP 不入门直接放弃",meta:[{property:"og:title",content:"NLP 不入门直接放弃"}]}),(f,c)=>(n(),t("div",i,s))}},m=r(p,[["__file","/home/runner/work/renovamen.github.io/renovamen.github.io/pages/posts/zh/2018-02-17-nlp-resource.md"]]);export{m as default,w as meta,k as tags,b as title};
