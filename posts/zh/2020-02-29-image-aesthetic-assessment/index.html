<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="icon" href="/favicon.png" type="image/png">
    <link rel="icon" href="/favicon.svg" type="image/svg+xml">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">

    <link rel="canonical" href="https://zxh.io/posts/zh/2020-02-29-image-aesthetic-assessment/">
    <meta name="generator" content="Astro v2.1.2">
    <meta name="msapplication-TileColor" content="#ffffff">

    <title>调研：图像美感评估 - Xiaohan Zou</title>

    <!-- General Meta Tags -->
    <meta name="title" content="调研：图像美感评估 - Xiaohan Zou">
    <meta name="description" content="A dragon lost in human world.">
    <meta name="author" content="Xiaohan Zou">

    <!-- Open Graph / Facebook -->
    <meta property="og:title" content="调研：图像美感评估 - Xiaohan Zou">
    <meta property="og:description" content="A dragon lost in human world.">
    <meta property="og:url" content="https://zxh.io/posts/zh/2020-02-29-image-aesthetic-assessment/">

    <!-- Twitter -->
    <meta property="twitter:title" content="调研：图像美感评估 - Xiaohan Zou">
    <meta property="twitter:description" content="A dragon lost in human world.">
    <meta property="twitter:url" content="https://zxh.io/posts/zh/2020-02-29-image-aesthetic-assessment/">

    
  <link rel="stylesheet" href="/_astro/404.0002c05b.css" />
<link rel="stylesheet" href="/_astro/index.30378a3b.css" />
<link rel="stylesheet" href="/_astro/_slug_.6ea23a96.css" /><script type="module" src="/_astro/hoisted.1b194908.js"></script></head>

  <body class="font-sans">
    <main class="post astro-GVPN4U4B flex flex-col min-h-full text-c px-4 pt-24 pb-6">
      <style>astro-island,astro-slot{display:contents}</style><script>(self.Astro=self.Astro||{}).load=a=>{(async()=>await(await a())())()},window.dispatchEvent(new Event("astro:load"));var l;{const c={0:t=>t,1:t=>JSON.parse(t,o),2:t=>new RegExp(t),3:t=>new Date(t),4:t=>new Map(JSON.parse(t,o)),5:t=>new Set(JSON.parse(t,o)),6:t=>BigInt(t),7:t=>new URL(t),8:t=>new Uint8Array(JSON.parse(t)),9:t=>new Uint16Array(JSON.parse(t)),10:t=>new Uint32Array(JSON.parse(t))},o=(t,s)=>{if(t===""||!Array.isArray(s))return s;const[e,n]=s;return e in c?c[e](n):void 0};customElements.get("astro-island")||customElements.define("astro-island",(l=class extends HTMLElement{constructor(){super(...arguments);this.hydrate=()=>{if(!this.hydrator||this.parentElement&&this.parentElement.closest("astro-island[ssr]"))return;const s=this.querySelectorAll("astro-slot"),e={},n=this.querySelectorAll("template[data-astro-template]");for(const r of n){const i=r.closest(this.tagName);!i||!i.isSameNode(this)||(e[r.getAttribute("data-astro-template")||"default"]=r.innerHTML,r.remove())}for(const r of s){const i=r.closest(this.tagName);!i||!i.isSameNode(this)||(e[r.getAttribute("name")||"default"]=r.innerHTML)}const a=this.hasAttribute("props")?JSON.parse(this.getAttribute("props"),o):{};this.hydrator(this)(this.Component,a,e,{client:this.getAttribute("client")}),this.removeAttribute("ssr"),window.removeEventListener("astro:hydrate",this.hydrate),window.dispatchEvent(new CustomEvent("astro:hydrate"))}}connectedCallback(){!this.hasAttribute("await-children")||this.firstChild?this.childrenConnectedCallback():new MutationObserver((s,e)=>{e.disconnect(),this.childrenConnectedCallback()}).observe(this,{childList:!0})}async childrenConnectedCallback(){window.addEventListener("astro:hydrate",this.hydrate);let s=this.getAttribute("before-hydration-url");s&&await import(s),this.start()}start(){const s=JSON.parse(this.getAttribute("opts")),e=this.getAttribute("client");if(Astro[e]===void 0){window.addEventListener(`astro:${e}`,()=>this.start(),{once:!0});return}Astro[e](async()=>{const n=this.getAttribute("renderer-url"),[a,{default:r}]=await Promise.all([import(this.getAttribute("component-url")),n?import(n):()=>()=>{}]),i=this.getAttribute("component-export")||"default";if(!i.includes("."))this.Component=a[i];else{this.Component=a;for(const d of i.split("."))this.Component=this.Component[d]}return this.hydrator=r,this.hydrate},s,this)}attributeChangedCallback(){this.hydrator&&this.hydrate()}},l.observedAttributes=["props"],l))}</script><astro-island uid="1e5Mru" data-solid-render-id="s0" component-url="/_astro/Navbar.d3083d1e.js" component-export="default" renderer-url="/_astro/client.8213e0be.js" props="{&quot;activePage&quot;:[0,&quot;posts&quot;]}" ssr="" client="load" opts="{&quot;name&quot;:&quot;Navbar&quot;,&quot;value&quot;:true}" await-children=""><header data-hk="s00-0" class="z-40 w-full h-14 hstack justify-between bg-c font-ui px-4 md:px-5 false false absolute top-0 left-0"><a class="font-bold" un-text="c-light hover:c-dark" href="/"><span text="lg">hi@zxh</span><div i-fa6-solid:angle-right class="prompt inline-block"></div><span class="blink">_</span></a><nav hstack space-x-4><a nav-item href="/projects" title="Projects"><div i-ph:rocket-launch-duotone class="md:hidden"></div><span class="lt-md:hidden false">Projects</span></a><a nav-item href="/posts" title="Blog"><div i-majesticons:pencil-line class="md:hidden"></div><span class="lt-md:hidden active">Blog</span></a><a nav-item href="/search" title="Search"><span i-uil:search></span></a><a nav-item href="/rss.xml" title="RSS" target="_blank" rel="noopener noreferrer"><div i-jam:rss-feed></div></a><button nav-item title="Toggle dark"><div i="carbon-sun dark:carbon-moon"></div></button><!--#--><astro-slot><astro-island uid="ZB3X02" data-solid-render-id="s1" component-url="/_astro/ToggleToc.ce6cb55a.js" component-export="default" renderer-url="/_astro/client.8213e0be.js" props="{&quot;slot&quot;:[0,&quot;navbar&quot;],&quot;class&quot;:[0,&quot;astro-GVPN4U4B&quot;]}" ssr="" client="load" opts="{&quot;name&quot;:&quot;ToggleToc&quot;,&quot;value&quot;:true}"></astro-island></astro-slot><!--/--></nav></header></astro-island>
      <div class="flex-1 mb-6">
        <div class="prose prose-lg mx-auto">
          <script>(self.Astro=self.Astro||{}).visible=(s,c,n)=>{const r=async()=>{await(await s())()};let i=new IntersectionObserver(e=>{for(const t of e)if(!!t.isIntersecting){i.disconnect(),r();break}});for(let e=0;e<n.children.length;e++){const t=n.children[e];i.observe(t)}},window.dispatchEvent(new Event("astro:visible"));</script><div mt-6 mb-8 class="astro-GVPN4U4B">
    <h1 text-4xl font-bold leading-12 my-0 class="astro-GVPN4U4B">调研：图像美感评估</h1>
    <p opacity-50 mt-2 class="astro-GVPN4U4B">
      Feb 29, 2020 · 19 min
      <span class="astro-GVPN4U4B">
            ·
            <span i-uil:tag-alt text-sm class="astro-GVPN4U4B"></span>
            <span class="astro-GVPN4U4B">
                <a href="/posts/zh/tags/computer-vision" class="!text-c astro-GVPN4U4B">
                  computer vision
                </a>
                
              </span>
          </span>
    </p>
  </div><div class="content mb-16 astro-GVPN4U4B">
    <ul class="table-of-contents">
<li>
<p><a href="#surveys">Surveys</a></p>
</li>
<li>
<p><a href="#datasets">Datasets</a></p>
<ul>
<li><a href="#photonet-pn">Photo.net (PN)</a></li>
<li><a href="#dpchallenge">DPChallenge</a></li>
<li><a href="#cuhk-pq">CUHK-PQ</a></li>
<li><a href="#ava">AVA</a></li>
<li><a href="#aadb">AADB</a></li>
<li><a href="#arod">AROD</a></li>
<li><a href="#gpa">GPA</a></li>
<li><a href="#pccd">PCCD</a></li>
<li><a href="#dpc-caption">DPC-Caption</a></li>
<li><a href="#%E5%85%B6%E4%BB%96">其他</a></li>
</ul>
</li>
<li>
<p><a href="#papers">Papers</a></p>
<ul>
<li><a href="#%E6%89%8B%E5%B7%A5%E7%89%B9%E5%BE%81">手工特征</a></li>
<li><a href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度学习</a></li>
</ul>
</li>
</ul>
<p>对图像美感评估（Image Aesthetic Assessment）领域的简单调研，到时候大概可以直接复制粘贴进毕业论文里。</p>
<h2 id="surveys"><a aria-hidden="true" tabindex="-1" class="header-anchor" href="#surveys">#</a>Surveys</h2>
<ul>
<li>
<p><strong>Algorithmic Inferencing of Aesthetics and Emotion in Natural Images: An Exposition.</strong> <em>Ritendra Datta, Jia Li, and James Z. Wang.</em> ICIP 2008. <a href="https://www.ri.cmu.edu/pub_files/pub4/datta_ritendra_2008_2/datta_ritendra_2008_2.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
</li>
<li>
<p><strong>On Aesthetics and Emotions in Images: A Computational Perspective.</strong> <em>Dhiraj Joshi, et al.</em> IEEE Signal Processing Magazine 2011. <a href="https://ieeexplore.ieee.org/document/5999579" target="_blank" rel="noopener noreferrer">[IEEE]</a> <a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/37213.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
</li>
<li>
<p><strong>Aesthetic Analysis of Images.</strong> 2015. <a href="https://eg.uc.pt/bitstream/10316/35507/1/Aesthetic%20Analysis%20of%20Images%20Intermediate%20report.pdf" target="_blank" rel="noopener noreferrer">[Report]</a></p>
</li>
<li>
<p><strong>Image Aesthetic Assessment: An Experimental Survey.</strong> <em>Yubin Deng, et al.</em> IEEE Signal Processing Magazine 2017. <a href="https://ieeexplore.ieee.org/abstract/document/7974874" target="_blank" rel="noopener noreferrer">[IEEE]</a> <a href="https://arxiv.org/pdf/1610.00838.pdf" target="_blank" rel="noopener noreferrer">[arxiv]</a></p>
</li>
<li>
<p><strong>A Report: Image Aesthetic Assessment.</strong> <em>Chunbiao Zhu.</em> 2018. <a href="https://www.researchgate.net/publication/325184839_A_Report_Image_Aesthetic_Assessment" target="_blank" rel="noopener noreferrer">[Report]</a></p>
</li>
</ul>
<h2 id="datasets"><a aria-hidden="true" tabindex="-1" class="header-anchor" href="#datasets">#</a>Datasets</h2>
<h3 id="photonet-pn"><a aria-hidden="true" tabindex="-1" class="header-anchor" href="#photonet-pn">#</a>Photo.net (PN)</h3>
<p><strong>Algorithmic Inferencing of Aesthetics and Emotion in Natural Images: An Exposition.</strong> <em>Ritendra Datta, Jia Li, and James Z. Wang.</em> ICIP 2008. <a href="https://www.ri.cmu.edu/pub_files/pub4/datta_ritendra_2008_2/datta_ritendra_2008_2.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
<p>共 20,278 张图片（<a href="https://arxiv.org/pdf/1610.00838.pdf" target="_blank" rel="noopener noreferrer">Deng et al. 2017</a>），平均每张图片 12 个评分，评分范围为 0-7，分越高说明图片质量越高。所有图片的平均得分的峰值在分布右侧，说明整体评价偏高。</p>
<p></p><figure alt="photo-net-distribution"><img src="/img/posts/zh/2020-02-29/photo-net-distribution.png" alt="photo-net-distribution"><figcaption><p>PN 数据集的图片平均得分分布</p></figcaption></figure><p></p>
<p>有论文认为 PN 数据集中超过 30% 的图片都被其拍摄者 P 上了一个相框以让它更好看一些，这会导致这些图片的评分偏高（<a href="https://ieeexplore.ieee.org/abstract/document/6126444" target="_blank" rel="noopener noreferrer">Marchesotti, et al. 2011</a>）。</p>
<p>下载：需要根据 <a href="http://ritendra.weebly.com/aesthetics-datasets.html" target="_blank" rel="noopener noreferrer">Dataset File</a> 中的索引号去 <a href="http://photo.net" target="_blank" rel="noopener noreferrer">photo.net</a> 上找图片，有的图片已经被从网上移除了。</p>
<h3 id="dpchallenge"><a aria-hidden="true" tabindex="-1" class="header-anchor" href="#dpchallenge">#</a>DPChallenge</h3>
<p><strong>Algorithmic Inferencing of Aesthetics and Emotion in Natural Images: An Exposition.</strong> <em>Ritendra Datta, Jia Li, and James Z. Wang.</em> ICIP 2008. <a href="https://www.ri.cmu.edu/pub_files/pub4/datta_ritendra_2008_2/datta_ritendra_2008_2.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a>（依然是这篇论文）</p>
<p>共 16,509 张图片，平均每张图片 205 个评分，评分范围为 0-10，分越高说明图片质量越高。</p>
<p></p><figure alt="dpchallenge-distribution"><img src="/img/posts/zh/2020-02-29/dpchallenge-distribution.png" alt="dpchallenge-distribution"><figcaption><p>DPChallenge 数据集的图片平均得分分布</p></figcaption></figure><p></p>
<p><strong>现在大概已经可以被 AVA 数据集所取代。</strong></p>
<p>下载：需要根据 <a href="http://ritendra.weebly.com/aesthetics-datasets.html" target="_blank" rel="noopener noreferrer">Dataset File</a> 中的索引号去 <a href="https://www.dpchallenge.com/" target="_blank" rel="noopener noreferrer">dpchallenge.com</a> 上找图片。</p>
<h3 id="cuhk-pq"><a aria-hidden="true" tabindex="-1" class="header-anchor" href="#cuhk-pq">#</a>CUHK-PQ</h3>
<p><strong>Content-based Photo Quality Assessment.</strong> <em>Wei Luo, Xiaogang Wang, and Xiaoou Tang.</em> ICCV 2011. <a href="https://ieeexplore.ieee.org/abstract/document/6126498" target="_blank" rel="noopener noreferrer">[IEEE]</a> <a href="http://mmlab.ie.cuhk.edu.hk/archive/2011/cvpr11_WLuo_XWang_XTang.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
<p><strong>Content-based Photo Quality Assessment.</strong> <em>Xiaoou Tang, Wei Luo, and Xiaogang Wang.</em> TMM 2013. <a href="https://ieeexplore.ieee.org/document/6544270" target="_blank" rel="noopener noreferrer">[IEEE]</a> <a href="http://mmlab.ie.cuhk.edu.hk/archive/2011/cvpr11_WLuo_XWang_XTang.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
<p>现在有 30,000 左右张图片，每张图片被按内容分为了 7 个类别。美学标注只有“高质量”和“低质量”两个，高低质量图片比例约为 1:3（<a href="https://arxiv.org/pdf/1610.00838.pdf" target="_blank" rel="noopener noreferrer">Deng et al. 2017</a>）。所以该数据集很难被用来进行评分任务的训练，而且分类难度不算大。因为是直接把从专业摄影网站（dpchallenge.com）和从业余摄影师处收集到的图片混在了一起，所以可能不能代表真实场景（<a href="http://refbase.cvc.uab.es/files/MMP2012a.pdf" target="_blank" rel="noopener noreferrer">Murray et al. 2012</a>）。</p>
<p>下载：<a href="http://mmlab.ie.cuhk.edu.hk/archive/CUHKPQ/Dataset.htm" target="_blank" rel="noopener noreferrer"><span i-gg:link=""></span> Multimedia Laboratory, CUHK</a></p>
<h3 id="ava"><a aria-hidden="true" tabindex="-1" class="header-anchor" href="#ava">#</a>AVA</h3>
<p><strong>AVA: A Large-Scale Database for Aesthetic Visual Analysis.</strong> <em>Naila Murray, Luca Marchesotti, Florent Perronnin.</em> CVPR 2012. <a href="https://ieeexplore.ieee.org/document/6247954" target="_blank" rel="noopener noreferrer">[IEEE]</a> <a href="http://refbase.cvc.uab.es/files/MMP2012a.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/mtobeiyf/ava_downloader" target="_blank" rel="noopener noreferrer">[Dataset]</a></p>
<p>收集并标注了来源于 dpchallenge.com 的超过 250,000 张图片。带有美学质量标注（Aesthetic annotations）（78 ～ 549 个评分，评分范围为 0-10，分越高说明图片质量越高）、语义标注（Semantic annotations）（共 66 种）和摄影风格标注（Photographic style annotations）（共 14 种，大概就是所谓的“美学因素”）。是第一个带有详细标注的大型图像美感评估数据集，也是图像美感评估领域公认的基准数据集。</p>
<p>该论文比较了 AVA 与其他数据集：</p>
<p></p><figure alt="compare"><img src="/img/posts/zh/2020-02-29/ava-compare.png" alt="compare"><figcaption><p>AVA 与其他数据集的比较</p></figcaption></figure><p></p>
<p>对于二分类任务，一般认为平均得分高于 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mo>+</mo><mi>σ</mi></mrow><annotation encoding="application/x-tex">5 + \sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span></span> 的图片质量高，低于 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mo>+</mo><mi>σ</mi></mrow><annotation encoding="application/x-tex">5 + \sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span></span> 的图片质量低。</p>
<p>但 AVA 的很多图片后期处理痕迹太重。且绝大多数图片来自于专业摄影者而非普通人，所以 AVA 有较大的 bias（<a href="https://arxiv.org/pdf/1606.01621.pdf" target="_blank" rel="noopener noreferrer">Kone et al. 2016</a>）。</p>
<p>下载：<a href="https://github.com/mtobeiyf/ava_downloader" target="_blank" rel="noopener noreferrer"><span i-gg:link=""></span> Github</a></p>
<h3 id="aadb"><a aria-hidden="true" tabindex="-1" class="header-anchor" href="#aadb">#</a>AADB</h3>
<p><strong>Photo Aesthetics Ranking Network with Attributes and Content Adaptation.</strong> <em>Shu Kong, et al.</em> ECCV 2016. <a href="https://arxiv.org/pdf/1606.01621.pdf" target="_blank" rel="noopener noreferrer">[arxiv]</a> <a href="https://www.ics.uci.edu/~skong2/aesthetics.html" target="_blank" rel="noopener noreferrer">[Project]</a> <a href="https://github.com/aimerykong/deepImageAestheticsAnalysis" target="_blank" rel="noopener noreferrer">[Code]</a> <a href="https://drive.google.com/open?id=0BxeylfSgpk1MOVduWGxyVlJFUHM" target="_blank" rel="noopener noreferrer">[Dataset &#x26; Model]</a></p>
<p>收集并标注了来自 <a href="https://www.flickr.com/" target="_blank" rel="noopener noreferrer">Flickr</a> 的共 10,000 张图片。带有美学质量标注（5 个人评分）和美学因素标注（共 11 种，二分类，算是对 AVA 的补充）。</p>
<p>相比 AVA，AADB 中所有图片都是真实照片，且注意平衡了专业摄影者和普通拍照者的作品数量，且标注了每个评分的标注者 ID（用于消除标不同标注者评价标准的不同所带来的影响）。</p>
<p>该论文还比较了 AADB 与其他数据集：</p>
<p></p><figure alt="compare"><img src="/img/posts/zh/2020-02-29/aadb-compare.png" alt="compare"><figcaption><p>AADB 与其他数据集的比较</p></figcaption></figure><p></p>
<p>但 AADB 的数据量过小，且标注人员过少。</p>
<p>下载：<a href="https://drive.google.com/open?id=0BxeylfSgpk1MOVduWGxyVlJFUHM" target="_blank" rel="noopener noreferrer"><span i-gg:link=""></span> Google Drive</a></p>
<h3 id="arod"><a aria-hidden="true" tabindex="-1" class="header-anchor" href="#arod">#</a>AROD</h3>
<p><strong>Will People Like Your Image? Learning the Aesthetic Space.</strong> <em>Katharina Schwarz, Patrick Wieschollek, and Hendrik P. A. Lensch.</em> WACV 2018. <a href="https://arxiv.org/pdf/1611.05203.pdf" target="_blank" rel="noopener noreferrer">[arxiv]</a> <a href="https://github.com/cgtuebingen/will-people-like-your-image" target="_blank" rel="noopener noreferrer">[Code &#x26; Dataset]</a></p>
<p>从 <a href="https://www.flickr.com/" target="_blank" rel="noopener noreferrer">Flickr</a> 上爬了 2004 - 2016 年间的超过 380,000 张图片，及他们的浏览量（views）、评论数、被喜欢次数（faves）、图片标题、在网站上的描述等数据。然后定义了图片的美学质量为：</p>
<div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>S</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>∼</mo><mfrac><mrow><mi>log</mi><mo>⁡</mo><mi>F</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow><mrow><mi>log</mi><mo>⁡</mo><mi>V</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">S(i) \sim \frac{\log F(i)}{\log V(i)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div>
<p>其中 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mclose">)</span></span></span></span></span> 为每张图片的 views，<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F(i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mclose">)</span></span></span></span></span> 为 faves。</p>
<p>相当于用一种类似于无监督标注的方法在试图解决 AVA 的 bias 太重和 AADB 数据量和标注人员过少的问题。</p>
<p>该论文还比较了 AROD 和 AVA 与 AADB：</p>
<p></p><figure alt="compare"><img src="/img/posts/zh/2020-02-29/arod-compare.png" alt="compare"><figcaption><p>AROD 与其他数据集的比较</p></figcaption></figure><p></p>
<p>下载：<a href="https://github.com/cgtuebingen/will-people-like-your-image" target="_blank" rel="noopener noreferrer"><span i-gg:link=""></span> Github</a>，<code>arod/list.txt</code> 中给出了图片的 URL，格式为 <code>url;faves;views</code>。</p>
<h3 id="gpa"><a aria-hidden="true" tabindex="-1" class="header-anchor" href="#gpa">#</a>GPA</h3>
<p><strong>Gourmet Photography Dataset for Aesthetic Assessment of Food Images.</strong> <em>Kekai Sheng, et al.</em> SIGGRAPH 2018. <a href="https://github.com/Openning07/GPA" target="_blank" rel="noopener noreferrer">[Dataset]</a></p>
<p>一个用于食物图片美感评估的数据集。从社交媒体和食物分类数据集中收集了共 24,000 张图片，然后人工标注（二分类）。</p>
<p>下载：<a href="https://github.com/Openning07/GPA" target="_blank" rel="noopener noreferrer"><span i-gg:link=""></span> Github</a></p>
<h3 id="pccd"><a aria-hidden="true" tabindex="-1" class="header-anchor" href="#pccd">#</a>PCCD</h3>
<p><strong>Aesthetic Critiques Generation for Photos.</strong> <em>Kuang-Yu Chang, Kung-Hung Lu, and Chu-Song Chen.</em> ICCV 2017. <a href="https://ieeexplore.ieee.org/document/8237642" target="_blank" rel="noopener noreferrer">[IEEE]</a> <a href="https://www.iis.sinica.edu.tw/~kuangyu/iccv17_aesthetic_critiques.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/kunghunglu/DeepPhotoCritic-ICCV17" target="_blank" rel="noopener noreferrer">[Code]</a> <a href="https://github.com/ivclab/DeepPhotoCritic-ICCV17" target="_blank" rel="noopener noreferrer">[Dataset]</a></p>
<p>首次在图像美学数据集中加入了语言评论信息。包含了来自 <a href="https://gurushots.com/" target="_blank" rel="noopener noreferrer">GuruShots</a>（一个专业图片评论网站）上的 4235 张图片，和它们的超过 60,000 条评论信息。评论被按角度（如“color &#x26; light”、“depth of field”等）分成了 7 类。并且张图片都带有对图片整体和 7 个角度的打分（评分范围为 1-10）。可以说在美学因素的标注上比 AVA 和 AADB 的二分类更详细。</p>
<p>但数据量过少。</p>
<p><del>我并没有找到这个数据集，该论文的 <a href="https://github.com/kunghunglu/DeepPhotoCritic-ICCV17" target="_blank" rel="noopener noreferrer">Github 主页</a>上似乎也没有。</del> 不久前似乎开源了：<a href="https://github.com/ivclab/DeepPhotoCritic-ICCV17" target="_blank" rel="noopener noreferrer"><span i-gg:link=""></span> Github</a></p>
<p>我也不明白为啥 2017 年的论文 2020 年才开源数据集…</p>
<h3 id="dpc-caption"><a aria-hidden="true" tabindex="-1" class="header-anchor" href="#dpc-caption">#</a>DPC-Caption</h3>
<p><strong>Aesthetic Attributes Assessment of Images.</strong> <em>Xin Jin, et al.</em> ACM MM 2019. <a href="https://arxiv.org/pdf/1907.04983.pdf" target="_blank" rel="noopener noreferrer">[arxiv]</a> <a href="https://github.com/BestiVictory/DPC-Captions" target="_blank" rel="noopener noreferrer">[Dataset]</a></p>
<p>把 AVA 中图片的评论都爬了下来，并按角度分成了 6 类，分类的标准基于从 PCCD 中统计出来的信息。比 PCCD 数据量大得多，但没有每个角度的评分，标注要弱一些。</p>
<p>下载：<a href="https://github.com/BestiVictory/DPC-Captions" target="_blank" rel="noopener noreferrer"><span i-gg:link=""></span> Github</a></p>
<h3 id="其他"><a aria-hidden="true" tabindex="-1" class="header-anchor" href="#其他">#</a>其他</h3>
<ul>
<li>
<p><strong>Understanding Aesthetics in Photography Using Deep Convolutional neural Networks.</strong> <em>Maciej Suchecki and Tomasz Trzciski.</em> SPA 2017. <a href="https://ieeexplore.ieee.org/abstract/document/8166855" target="_blank" rel="noopener noreferrer">[IEEE]</a> <a href="http://ii.pw.edu.pl/~ttrzcins/papers/SPA_2017.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
<p>该论文用的数据集与 AROD 比较类似，从 Flicker 上爬了大概 170 万张图片，定义的美学分数为：</p>
<div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>score</mtext><mo>=</mo><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><msub><mi>n</mi><mtext>views</mtext></msub><mo>+</mo><mfrac><mn>1</mn><msub><mi>n</mi><mtext>days</mtext></msub></mfrac><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{score} = \log_2 (n_{\text{views}} + \frac{1}{n_{\text{days}}} + 1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord text"><span class="mord">score</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">views</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.2935em;vertical-align:-0.9721em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">days</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9721em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span></div>
<p>其中 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mtext>views</mtext></msub></mrow><annotation encoding="application/x-tex">n_{\text{views}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">views</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> 是图片浏览量，<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mtext>days</mtext></msub></mrow><annotation encoding="application/x-tex">n_{\text{days}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">days</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span> 是距离图片上传日期的天数。</p>
<p>然而该论文最后给出的数据集地址已经失效了…</p>
</li>
<li>
<p><strong>An Image is Worth More than a Thousand Favorites: Surfacing the Hidden Beauty of Flickr Pictures.</strong> <em>Rossano Schifanella, et al.</em> ICWSM 2015. <a href="https://arxiv.org/pdf/1505.03358.pdf" target="_blank" rel="noopener noreferrer">[arxiv]</a> <a href="http://www.di.unito.it/~schifane/dataset/beauty-icwsm15/#" target="_blank" rel="noopener noreferrer">[Dataset]</a></p>
<p>研究的是在人气较低的图片中找出高质量图片，它的数据集包含来自 Flicker 上的约 15,000 张图片，带有语义标注（共 4 种）和美学质量标注（评分范围为 1-5）。</p>
<p>下载：需要根据<a href="http://www.di.unito.it/~schifane/dataset/beauty-icwsm15/#" target="_blank" rel="noopener noreferrer">这里</a>的 <code>flickr_photo_id</code> 通过 <a href="https://www.flickr.com/services/api/" target="_blank" rel="noopener noreferrer">Flicker API</a> 去搞图片。</p>
</li>
</ul>
<h2 id="papers"><a aria-hidden="true" tabindex="-1" class="header-anchor" href="#papers">#</a>Papers</h2>
<h3 id="手工特征"><a aria-hidden="true" tabindex="-1" class="header-anchor" href="#手工特征">#</a>手工特征</h3>
<p><strong>全局特征：</strong></p>
<ul>
<li>
<p><strong>Classification of Digital Photos Taken by Photographers or Home Users.</strong> <em>Hanghang Tong, et al.</em> PCM 2004.
<a href="http://bigeye.au.tsinghua.edu.cn/english/paper/_PCM04_tong.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
<p>图像美感评估最早的一篇论文，提取了基本的全局低级特征（blurriness、contrast、colorfulness、saliency），然后用 boosting 来区分来自专业摄影师和普通拍照者的照片。</p>
</li>
<li>
<p><strong>Studying Aesthetics in Photographic Images Using a Computational Approach.</strong> <em>Ritendra Datta, et al.</em> ECCV 2006. <a href="http://infolab.stanford.edu/~wangz/project/imsearch/Aesthetics/ECCV06/datta.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
<p>靠直觉（论文原话）提取了 56 维低层和高层特征，用 SVM 搞二分类，用 CART 评估美感分数值。展示了用机器学习进行图像美感分析的可行性，是个里程碑。</p>
</li>
<li>
<p><strong>The Design of High-Level Features for Photo Quality Assessment.</strong> <em>Yan Ke, Xiaoou Tang, Feng Jing.</em> CVPR 2006. <a href="http://www-cgi.cs.cmu.edu/~yke/photoqual/cvpr06photo.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
<p>构造了包括很多高层语义特征在内的 7 维特征，然后用朴素贝叶斯搞二分类。该论文跟上一篇论文都是最早把图像审美问题转换为二分类问题的尝试。</p>
</li>
</ul>
<p><strong>前景背景对比特征：</strong></p>
<ul>
<li>
<p><strong>Photo and Video Quality Evaluation: Focusing on the Subject.</strong> <em>Yiwen Luo and Xiaoou Tang.</em> ECCV 2008. <a href="http://mmlab.ie.cuhk.edu.hk/pdf/luoT_ECCV08.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
<p>先把主题区域和背景分离，然后考虑了主题区域和背景的清晰度对比度、亮度对比度等特征，最终提取了 5 维特征。分类器分别使用了朴素贝叶斯、SVM 和 Gentle AdaBoost。从它给出的结果来看效果比上面两篇论文提高了很多。它还顺便用这种方法搞了一下视频质量评估。</p>
</li>
<li>
<p><strong>Saliency-enhanced Image Aesthetics Class Prediction.</strong> <em>Lai-Kuan Wong, Kok-Lim Low.</em> ICIP 2009. <a href="https://ieeexplore.ieee.org/document/5413825" target="_blank" rel="noopener noreferrer">[IEEE]</a> <a href="https://projet.liris.cnrs.fr/imagine/pub/proceedings/ICIP-2009/pdfs/0000997.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
<p>也分离了主题区域和背景，认为主题区域一定是显著区域，所以在提取了主题区域时用了一个显著性模型。而且在考虑前景背景对比特征的同时也考虑了一些全局特征。然后用 SVM 搞二分类。</p>
</li>
<li>
<p><strong>A Framework for Photo-quality Assessment and Enhancement Based on Visual Aesthetics.</strong> <em>Subhabrata Bhattacharya, et al.</em> ACM MM 2010. <a href="http://www.cs.cmu.edu/~rahuls/pub/mm2010-rahuls.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
<p><strong>A Holistic Approach to Aesthetic Enhancement of Photographs.</strong> <em>Subhabrata Bhattacharya, et al.</em> TOMCCAP 2011. <a href="http://www.cs.cmu.edu/~rahuls/pub/tomccap2011-rahuls.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
<p>对有单个前景的图片提取了 relative foreground position 作为特征（用分割或显著度提取前景的 visual attention point，然后算它跟四个 stress point 之间的距离），对风景图片提取了 visual weight ratio 作为特征（检测地平线，然后算它跟黄金分割线的差距）。然后用 SVR 来对图片的的美感进行打分（范围为 1-5）。它还能根据上述法则提出增强图片美感的建议。</p>
</li>
</ul>
<p><strong>通用局部特征：</strong></p>
<ul>
<li>
<p><strong>Aesthetic Quality Classification of Photographs Based on Color Harmony.</strong> <em>Masashi Nishiyama, et al.</em> CVPR 2011. <a href="http://research.nii.ac.jp/~imarik/resources/papers/CVPR2012-Nishiyama.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
</li>
<li>
<p><strong>Assessing the Aesthetic Quality of Photographs Using Generic Image Descriptors.</strong> <em>Luca Marchesotti, et al.</em> ICCV 2011. <a href="https://ieeexplore.ieee.org/document/6126444" target="_blank" rel="noopener noreferrer">[IEEE]</a> <a href="http://www.tamaraberg.com/teaching/Fall_13/papers/Marchesotti2011.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
</li>
<li>
<p><strong>A Statistic Approach for Photo Quality Assessment.</strong> <em>Li-Yun Lo, et al.</em> ISIC 2012. <a href="https://ieeexplore.ieee.org/abstract/document/6449719" target="_blank" rel="noopener noreferrer">[IEEE]</a> <a href="http://www.csie.kuas.edu.tw/~jcchen/pdf/A%20Statistic%20Approach%20for%20Photo%20Quality%20Assessment%20.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
</li>
</ul>
<h3 id="深度学习"><a aria-hidden="true" tabindex="-1" class="header-anchor" href="#深度学习">#</a>深度学习</h3>
<h4 id="分类"><a aria-hidden="true" tabindex="-1" class="header-anchor" href="#分类">#</a>分类</h4>
<p><strong>Multi-column：</strong></p>
<ul>
<li>
<p><strong>RAPID: Rating Pictorial Aesthetics Using Deep Learning.</strong> <em>Xin Lu, et al.</em> ACM MM 2014. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.710.1251&#x26;rep=rep1&#x26;type=pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
<p><strong>Rating Image Aesthetics Using Deep Learning.</strong> <em>Xin Lu, et al.</em> TMM 2015. <a href="https://ieeexplore.ieee.org/document/7243357" target="_blank" rel="noopener noreferrer">[IEEE]</a> <a href="http://infolab.stanford.edu/~wangz/project/imsearch/Aesthetics/TMM15/lu.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
<p>首次用神经网络搞图片美感评估。用了双列 CNN，把全局图像和随机提取出的一个局部 patch 各输入一列 CNN，输出的特征会被联合起来输入分类层进行二分类：</p>
<p><img src="/img/posts/zh/2020-02-29/rapid-dcnn.png" alt="rapid-dcnn"></p>
<p>因为数据集用的是有 style 标注的 AVA，该论文还把 style 属性也输入了一列 CNN，相当于最后是 3 列 CNN：</p>
<p><img src="/img/posts/zh/2020-02-29/rapid-style-cnn.png" alt="rapid-style-cnn"></p>
</li>
<li>
<p><strong>Deep Multi-Patch Aggregation Network for Image Style, Aesthetics, and Quality Estimation.</strong> <em>Xin Lu, et al.</em> ICCV 2015. <a href="https://ieeexplore.ieee.org/abstract/document/7410476/" target="_blank" rel="noopener noreferrer">[IEEE]</a> <a href="http://infolab.stanford.edu/~wangz/project/imsearch/Aesthetics/ICCV15/lu.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
<p>只用了 patch 作为输入。从图片中随机提取一些 patch，用一组 CNN 对每个 patch 提取特征，然后把所有 patch 的特征聚合起来，最后把特征输进 soft-max 层进行二分类。</p>
<p><img src="/img/posts/zh/2020-02-29/dma-net.png" alt="dma-net"></p>
</li>
<li>
<p><strong>A-Lamp: Adaptive Layout-Aware Multi-Patch Deep Convolutional Neural.</strong> <em>Shuang Ma, et al.</em> CVPR 2017. <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Ma_A-Lamp_Adaptive_Layout-Aware_CVPR_2017_paper.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://arxiv.org/pdf/1704.00248.pdf" target="_blank" rel="noopener noreferrer">[arxiv]</a></p>
<p>上一篇论文的 patch 是随机提取的，它们可能大量重叠，或者不能覆盖图片的关键信息。所以该论文用了一个显著性模型提取了显著度最高、更多样、重叠部分更少的一些 patch。</p>
<p>同时该论文还用了另一个显著性模型来提取出图片中的主要物体，从而利用了图片中的物体布局信息。</p>
<p><img src="/img/posts/zh/2020-02-29/alamp.png" alt="alamp"></p>
</li>
<li>
<p><strong>Attention-based Multi-Patch Aggregation for Image Aesthetic Assessment.</strong> <em>Kekai Sheng, et al.</em> ACM MM 2018. <a href="http://chongyangma.com/publications/am/2018_am_paper.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/Openning07/MPADA" target="_blank" rel="noopener noreferrer">[Code]</a></p>
<p>在聚合不同 patch 的特征时使用了 attention 机制。</p>
</li>
<li>
<p><strong>Brain-Inspired Deep Networks for Image Aesthetics Assessment.</strong> <em>Zhangyang Wang, et al.</em> arXiv 2016. <a href="https://arxiv.org/pdf/1601.04155.pdf" target="_blank" rel="noopener noreferrer">[arxiv]</a></p>
</li>
<li>
<p><strong>Composition-preserving Deep Photo Aesthetics Assessment.</strong> <em>Long Mai, et al.</em> CVPR 2016.
<a href="https://ieeexplore.ieee.org/document/7780429" target="_blank" rel="noopener noreferrer">[IEEE]</a> <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Mai_Composition-Preserving_Deep_Photo_CVPR_2016_paper.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="http://web.cecs.pdx.edu/~fliu/project/deep-quality/" target="_blank" rel="noopener noreferrer">[Project]</a></p>
<p>因为全连接层的特征数是固定的，所以 CNN 一般会对输入图片的尺寸做固定要求。为了适应这种要求，往往需要对输入图片进行剪裁拉伸等操作，这就会破坏图片的布局、降低图片分辨率、导致图片失真等，从而破坏图片的美感。</p>
<p>所以该论文提出了一个叫 Multi-Net Adaptive Spatial Pooling ConvNet 的结构，由多个用 adaptive spatial pooling 层替换了普通 pooling 层（卷积层和全连接层之间）的 CNN 组成，每个 CNN 除了 adaptive spatial pooling 不一样其他都一样。每个 CNN 的输入是同一张原始图片，输出是指定大小的特征。最后把所有网络输出的特征聚合起来。</p>
<p>然后还用了另外一个 CNN 来提取图片的场景特征，跟上述特征一起扔进聚合层聚合。</p>
<p><img src="/img/posts/zh/2020-02-29/mna.png" alt="mna"></p>
<p>跟 SPP 有点像：</p>
<p><strong>Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition.</strong> <em>Kaiming He, et al.</em> TPAMI 2015. <a href="https://ieeexplore.ieee.org/abstract/document/7005506" target="_blank" rel="noopener noreferrer">[IEEE]</a> <a href="https://arxiv.org/pdf/1406.4729.pdf" target="_blank" rel="noopener noreferrer">[arxiv]</a> <a href="https://github.com/yueruchen/sppnet-pytorch" target="_blank" rel="noopener noreferrer">[Code]</a></p>
</li>
</ul>
<p><strong>Multi-task：</strong></p>
<ul>
<li>
<p><strong>Deep Aesthetic Quality Assessment with Semantic Information.</strong> <em>Yueying Kao, Ran He, and Kaiqi Huang.</em> TIP 2017. <a href="https://ieeexplore.ieee.org/abstract/document/7814292" target="_blank" rel="noopener noreferrer">[IEEE]</a> <a href="https://arxiv.org/pdf/1604.04970.pdf" target="_blank" rel="noopener noreferrer">[arxiv]</a></p>
<p>用一个 MTCNN 来提取提取美学特征和场景语义信息。</p>
</li>
<li>
<p><strong>Hierarchical Aesthetic Quality Assessment Using Deep Convolutional Neural Networks.</strong> <em>Yueying Kao, Kaiqi Huang, and Steve Maybank.</em> Signal Processing: Image Communication 2016. <a href="https://core.ac.uk/download/pdf/141224862.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
</li>
<li>
<p><strong>Visual Aesthetic Quality Assessment with Multi-task Deep Learning.</strong> <em>Yueying Kao, Ran He, and Kaiqi Huang.</em> arXiv 2016. <a href="https://www.researchgate.net/profile/Yueying_Kao/publication/301877404_Visual_Aesthetic_Quality_Assessment_with_Multi-task_Deep_Learning/links/573a717108ae9f741b2cad7a/Visual-Aesthetic-Quality-Assessment-with-Multi-task-Deep-Learning.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
</li>
<li>
<p><strong>Photo Aesthetics Ranking Network with Attributes and Content Adaptation.</strong> <em>Shu Kong, et al.</em> ECCV 2016. <a href="https://arxiv.org/pdf/1606.01621.pdf" target="_blank" rel="noopener noreferrer">[arxiv]</a> <a href="https://www.ics.uci.edu/~skong2/aesthetics.html" target="_blank" rel="noopener noreferrer">[Project]</a> <a href="https://github.com/aimerykong/deepImageAestheticsAnalysis" target="_blank" rel="noopener noreferrer">[Code]</a> <a href="https://drive.google.com/open?id=0BxeylfSgpk1MOVduWGxyVlJFUHM" target="_blank" rel="noopener noreferrer">[Dataset &#x26; Model]</a></p>
<p>提出了 AADB 数据集。</p>
</li>
</ul>
<p><strong>DBM：</strong></p>
<ul>
<li>
<p><strong>Joint Image and Text Representation for Aesthetics Analysis.</strong> <em>Ye Zhou, et al.</em> ACM MM 2016. <a href="http://infolab.stanford.edu/~wangz/project/imsearch/Aesthetics/ACMMM2016/zhou.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a></p>
<p>把 AVA 中所有图片的评论都爬了下来，建了一个叫 AVA-Comments 的数据集。用 DBM 来同时利用图片的视觉特征和评论的文本特征。</p>
</li>
</ul>
<h4 id="评分分布"><a aria-hidden="true" tabindex="-1" class="header-anchor" href="#评分分布">#</a>评分分布</h4>
<ul>
<li>
<p><strong>NIMA: Neural Image Assessment.</strong> <em>Hossein Talebi and Peyman Milanfar.</em> TIP 2018. <a href="https://arxiv.org/pdf/1709.05424.pdf" target="_blank" rel="noopener noreferrer">[arxiv]</a> <a href="https://ai.googleblog.com/2017/12/introducing-nima-neural-image-assessment.html" target="_blank" rel="noopener noreferrer">[Blog]</a> <a href="https://github.com/titu1994/neural-image-assessment" target="_blank" rel="noopener noreferrer">[Code (Reproduction)]</a></p>
</li>
<li>
<p><strong>Predicting Aesthetic Score Distribution through Cumulative Jensen-Shannon Divergence.</strong> <em>Xin Jin, et al.</em> AAAI 2018. <a href="http://jinxin.me/downloads/papers/028-AAAI2018/ScoreDestribution.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/BestiVictory/CJS-CNN" target="_blank" rel="noopener noreferrer">[Code]</a></p>
</li>
</ul>
<h4 id="文字描述"><a aria-hidden="true" tabindex="-1" class="header-anchor" href="#文字描述">#</a>文字描述</h4>
<ul>
<li>
<p><strong>Aesthetic Critiques Generation for Photos.</strong> <em>Kuang-Yu Chang, Kung-Hung Lu, and Chu-Song Chen.</em> ICCV 2017. <a href="https://ieeexplore.ieee.org/document/8237642" target="_blank" rel="noopener noreferrer">[IEEE]</a> <a href="https://www.iis.sinica.edu.tw/~kuangyu/iccv17_aesthetic_critiques.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/kunghunglu/DeepPhotoCritic-ICCV17" target="_blank" rel="noopener noreferrer">[Code]</a> <a href="https://github.com/ivclab/DeepPhotoCritic-ICCV17" target="_blank" rel="noopener noreferrer">[Dataset]</a></p>
<p>在每个角度的数据集上跑了一个图像描述模型（直接用了 <a href="#show-and-tell">NeuralTalk2</a>），然后用 Attention 把所有模型输出的隐状态融合起来，作为另一个新的 LSTM 的输入，新的 LSTM 的输出就是涵盖了多个角度的描述。（但从论文给的结果来看角度还是比较单一？）</p>
<p></p><figure alt="pccd"><img src="/img/posts/zh/2020-02-29/pccd.png" alt="pccd" width="400"></figure><p></p>
</li>
<li>
<p><strong>Aesthetic Attributes Assessment of Images.</strong> <em>Xin Jin, et al.</em> ACM MM 2019. <a href="https://arxiv.org/pdf/1907.04983.pdf" target="_blank" rel="noopener noreferrer">[arxiv]</a> <a href="https://github.com/BestiVictory/DPC-Captions" target="_blank" rel="noopener noreferrer">[Dataset]</a></p>
<p>搞了一个比 PCCD 大得多的数据集，跑了一个浮夸的模型，输出指定角度的描述。</p>
<p><img src="/img/posts/zh/2020-02-29/aman.png" alt="aman"></p>
</li>
<li>
<p><strong>Neural Aesthetic Image Reviewer.</strong> <em>Wenshan Wang, et al.</em> IET Computer Vision 2019. <a href="https://arxiv.org/pdf/1802.10240.pdf" target="_blank" rel="noopener noreferrer">[arxiv]</a></p>
</li>
<li>
<p><strong>Aesthetic Image Captioning From Weakly-Labelled Photographs.</strong> <em>Koustav Ghosal, Aakanksha Rana, and Aljosa Smolic.</em> ICCV Workshop 2019. <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/CROMOL/Ghosal_Aesthetic_Image_Captioning_From_Weakly-Labelled_Photographs_ICCVW_2019_paper.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/V-Sense/Aesthetic-Image-Captioning-ICCVW-2019" target="_blank" rel="noopener noreferrer">[Code]</a></p>
</li>
</ul>
  </div><div class="grid md:grid-cols-2 text-[0.95em] astro-GVPN4U4B">
    <a href="https://github.com/Renovamen/renovamen.github.io/edit/main/src/content/blog/zh/2020-02-29-image-aesthetic-assessment.md" title="Edit link" target="_blank" rel="noopener noreferrer" class="hover:underline astro-GVPN4U4B">
      <span class="i-tabler:edit w-4.5 h-4.5 align-text-top astro-GVPN4U4B"></span>
      Edit this page on GitHub
    </a>
    <span md:text-right text-c-light class="astro-GVPN4U4B">
      <span class="i-ic:round-update w-4.5 h-4.5 astro-GVPN4U4B"></span>
      Last updated:
      3/7/2023, 2:57:21 PM
    </span>
  </div><div class="grid md:grid-cols-2 mt-3 pt-3 text-[0.95em] astro-GVPN4U4B" border="t c">
        <span class="prev astro-GVPN4U4B">
          <a href="/posts/zh/2020-02-24-the-enigmatic-appeal-of-video-games-greatest-bards" class="!text-c astro-GVPN4U4B">
              「译」游戏中的游吟诗人的神秘魅力
            </a>
        </span>
        <span class="next text-right astro-GVPN4U4B">
          <a href="/posts/zh/2020-03-17-papers-reading" class="!text-c astro-GVPN4U4B">
              三月大锅烩：机器翻译 / 文本分类 / 图像描述
            </a>
        </span>
      </div><div mt-20 class="astro-GVPN4U4B">
    <astro-island uid="ZouB9q" data-solid-render-id="s0" component-url="/_astro/Giscus.b37239bf.js" component-export="default" renderer-url="/_astro/client.8213e0be.js" props="{&quot;class&quot;:[0,&quot;astro-GVPN4U4B&quot;]}" ssr="" client="visible" opts="{&quot;name&quot;:&quot;Giscus&quot;,&quot;value&quot;:true}" await-children=""><div data-hk="s00-0" class="giscus"></div></astro-island>
  </div>
        </div>
      </div>
      <footer font="ui" text="sm center c-lighter" mt-20>
  © Xiaohan Zou 2023 <br>
  A dragon lost in human world
</footer>
    </main>
  </body></html>

