<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" href="/favicon.png" type="image/png"><link rel="icon" href="/favicon.svg" type="image/svg+xml"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><meta name="msapplication-TileColor" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin=""><script>!function(){var e=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,t=localStorage.getItem("vueuse-color-scheme")||"auto";("dark"===t||e&&"light"!==t)&&document.documentElement.classList.toggle("dark",!0)}()</script><script type="module" async="" crossorigin="" src="/assets/app-c4e7edd9.js"></script><style>.prompt[data-v-4d808a77]{vertical-align:-.2em;font-size:.85em}.blink[data-v-4d808a77]{animation:blinker-4d808a77 1s none infinite}@keyframes blinker-4d808a77{50%{opacity:0}}*,:after,:before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e5e7eb}html{line-height:1.5;-webkit-text-size-adjust:100%;-moz-tab-size:4;tab-size:4;font-family:ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,"Apple Color Emoji","Segoe UI Emoji",Segoe UI Symbol,"Noto Color Emoji"}body{margin:0;line-height:inherit}h1,h2,h3{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}strong{font-weight:bolder}code{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1em}button{font-family:inherit;font-size:100%;font-weight:inherit;line-height:inherit;color:inherit;margin:0;padding:0}button{text-transform:none}button{-webkit-appearance:button;background-color:transparent;background-image:none}figure,h1,h2,h3,p{margin:0}ul{list-style:none;margin:0;padding:0}button{cursor:pointer}img,svg{display:block;vertical-align:middle}img{max-width:100%;height:auto}*,:after,:before{--un-rotate:0;--un-rotate-x:0;--un-rotate-y:0;--un-rotate-z:0;--un-scale-x:1;--un-scale-y:1;--un-scale-z:1;--un-skew-x:0;--un-skew-y:0;--un-translate-x:0;--un-translate-y:0;--un-translate-z:0;--un-scroll-snap-strictness:proximity;--un-border-spacing-x:0;--un-border-spacing-y:0;--un-ring-offset-shadow:0 0 rgba(0,0,0,0);--un-ring-shadow:0 0 rgba(0,0,0,0);--un-shadow:0 0 rgba(0,0,0,0);--un-ring-offset-width:0px;--un-ring-offset-color:#fff;--un-ring-width:0px;--un-ring-color:rgba(147,197,253,.5)}@font-face{font-family:DM Sans;font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/dmsans/v11/rP2Hp2ywxg089UriCZ2IHSeH.woff2) format("woff2");unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:DM Sans;font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/dmsans/v11/rP2Hp2ywxg089UriCZOIHQ.woff2) format("woff2");unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:DM Sans;font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/dmsans/v11/rP2Cp2ywxg089UriASitCBamC2QX.woff2) format("woff2");unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:DM Sans;font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/dmsans/v11/rP2Cp2ywxg089UriASitCBimCw.woff2) format("woff2");unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}[i-fa6-solid\:angle-right=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 320 512' display='inline-block' vertical-align='sub' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256L73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;display:inline-block;vertical-align:sub;width:1.2em;height:1.2em}[i-ic\:round-update=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' display='inline-block' vertical-align='sub' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M11 8.75v3.68c0 .35.19.68.49.86l3.12 1.85c.36.21.82.09 1.03-.26c.21-.36.1-.82-.26-1.03l-2.87-1.71v-3.4c-.01-.4-.35-.74-.76-.74s-.75.34-.75.75zm10 .75V4.21c0-.45-.54-.67-.85-.35l-1.78 1.78a9.01 9.01 0 0 0-7.21-2.6c-4.19.38-7.64 3.75-8.1 7.94a9 9 0 0 0 17.87 2.14c.07-.6-.4-1.12-1-1.12c-.5 0-.92.37-.98.86c-.43 3.49-3.44 6.19-7.05 6.14c-3.71-.05-6.84-3.18-6.9-6.9C4.94 8.2 8.11 5 12 5c1.93 0 3.68.79 4.95 2.05l-2.09 2.09c-.32.32-.1.86.35.86h5.29c.28 0 .5-.22.5-.5z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;display:inline-block;vertical-align:sub;width:1.2em;height:1.2em}[i-jam\:rss-feed=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='-4 -4 24 24' display='inline-block' vertical-align='sub' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M1.996 15.97a1.996 1.996 0 1 1 0-3.992a1.996 1.996 0 0 1 0 3.992zM1.12 7.977a.998.998 0 0 1-.247-1.98a8.103 8.103 0 0 1 9.108 8.04v.935a.998.998 0 1 1-1.996 0v-.934a6.108 6.108 0 0 0-6.865-6.06zM0 1.065A.998.998 0 0 1 .93.002C8.717-.517 15.448 5.374 15.967 13.16c.042.626.042 1.254 0 1.88a.998.998 0 1 1-1.992-.133c.036-.538.036-1.077 0-1.614C13.53 6.607 7.75 1.548 1.065 1.994A.998.998 0 0 1 0 1.064z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;display:inline-block;vertical-align:sub;width:1.2em;height:1.2em}[i-majesticons\:pencil-line=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' display='inline-block' vertical-align='sub' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cg fill='currentColor'%3E%3Cpath d='M15.586 3a2 2 0 0 1 2.828 0L21 5.586a2 2 0 0 1 0 2.828l-12 12A2 2 0 0 1 7.586 21H5a2 2 0 0 1-2-2v-2.586A2 2 0 0 1 3.586 15l12-12zm-.172 3L18 8.586L19.586 7L17 4.414L15.414 6zm1.172 4L14 7.414l-9 9V19h2.586l9-9z'/%3E%3C/g%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;display:inline-block;vertical-align:sub;width:1.2em;height:1.2em}[i-ph\:rocket-launch-duotone=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 256 256' display='inline-block' vertical-align='sub' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M94.1 184.6c-11.4 33.9-56.6 33.9-56.6 33.9s0-45.2 33.9-56.6Zm90.5-67.9v64.6a8 8 0 0 1-2.4 5.6l-32.3 32.4a8 8 0 0 1-13.5-4.1l-8.4-41.9Zm-45.3-45.3H74.7a8 8 0 0 0-5.6 2.4l-32.4 32.3a8 8 0 0 0 4.1 13.5l41.9 8.4Z' opacity='.2'/%3E%3Cpath fill='currentColor' d='M96.6 177a7.9 7.9 0 0 0-10.1 5c-6.6 19.7-27.9 25.8-40.2 27.7c1.9-12.3 8-33.6 27.7-40.2a8 8 0 1 0-5.1-15.1c-16.4 5.4-28.4 18.4-34.8 37.5a91.8 91.8 0 0 0-4.6 26.6a8 8 0 0 0 8 8a91.8 91.8 0 0 0 26.6-4.6c19.1-6.4 32.1-18.4 37.5-34.8a7.9 7.9 0 0 0-5-10.1Z'/%3E%3Cpath fill='currentColor' d='M227.6 41.8a15.7 15.7 0 0 0-13.4-13.4c-11.3-1.7-40.6-2.5-69.2 26.1l-9 8.9H74.7a16.2 16.2 0 0 0-11.3 4.7l-32.3 32.4a15.9 15.9 0 0 0-4 15.9a16 16 0 0 0 12.2 11.1l39.5 7.9l41.8 41.8l7.9 39.5a16 16 0 0 0 11.1 12.2a14.7 14.7 0 0 0 4.6.7a15.6 15.6 0 0 0 11.3-4.7l32.4-32.3a16.2 16.2 0 0 0 4.7-11.3V120l8.9-9c28.6-28.6 27.8-57.9 26.1-69.2ZM74.7 79.4H120l-39.9 39.9l-37.7-7.5Zm81.6-13.6c7.8-7.8 28.8-25.6 55.5-21.6c4 26.7-13.8 47.7-21.6 55.5L128 161.9L94.1 128Zm20.3 115.5l-32.4 32.3l-7.5-37.7l39.9-39.9Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;display:inline-block;vertical-align:sub;width:1.2em;height:1.2em}[i-tabler\:edit=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' display='inline-block' vertical-align='sub' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cg fill='none' stroke='currentColor' stroke-linecap='round' stroke-linejoin='round' stroke-width='2'%3E%3Cpath d='M7 7H6a2 2 0 0 0-2 2v9a2 2 0 0 0 2 2h9a2 2 0 0 0 2-2v-1'/%3E%3Cpath d='M20.385 6.585a2.1 2.1 0 0 0-2.97-2.97L9 12v3h3l8.385-8.415zM16 5l3 3'/%3E%3C/g%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;display:inline-block;vertical-align:sub;width:1.2em;height:1.2em}[i-uil\:tag-alt=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' display='inline-block' vertical-align='sub' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M7.5 6A1.5 1.5 0 1 0 9 7.5A1.5 1.5 0 0 0 7.5 6Zm13.62 4.71l-8.41-8.42A1 1 0 0 0 12 2H3a1 1 0 0 0-1 1v9a1 1 0 0 0 .29.71l8.42 8.41a3 3 0 0 0 4.24 0L21.12 15a3 3 0 0 0 0-4.24Zm-1.41 2.82l-6.18 6.17a1 1 0 0 1-1.41 0L4 11.59V4h7.59l8.12 8.12a1 1 0 0 1 .29.71a1 1 0 0 1-.29.7Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;display:inline-block;vertical-align:sub;width:1.2em;height:1.2em}[i~=carbon-sun]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 32 32' display='inline-block' vertical-align='sub' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6ZM5.394 6.813L6.81 5.399l3.505 3.506L8.9 10.319zM2 15.005h5v2H2zm3.394 10.193L8.9 21.692l1.414 1.414l-3.505 3.506zM15 25.005h2v5h-2zm6.687-1.9l1.414-1.414l3.506 3.506l-1.414 1.414zm3.313-8.1h5v2h-5zm-3.313-6.101l3.506-3.506l1.414 1.414l-3.506 3.506zM15 2.005h2v5h-2z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;display:inline-block;vertical-align:sub;width:1.2em;height:1.2em}.prose :where(h1,h2,h3,h4,h5,h6):not(:where(.not-prose,.not-prose*)){color:var(--un-prose-headings);font-weight:600;line-height:1.25}.prose :where(a):not(:where(.not-prose,.not-prose*)){color:var(--un-prose-links);text-decoration:underline;font-weight:500}.prose :where(p,ul,ol,pre):not(:where(.not-prose,.not-prose*)){margin:1em 0;line-height:1.75}.prose :where(h2):not(:where(.not-prose,.not-prose*)){margin:1.75em 0 .5em;font-size:1.75em}.prose :where(h3):not(:where(.not-prose,.not-prose*)){margin:1.5em 0 .5em;font-size:1.375em}.prose :where(img,video):not(:where(.not-prose,.not-prose*)){max-width:100%}.prose :where(figure,picture):not(:where(.not-prose,.not-prose*)){margin:1em 0}.prose :where(figcaption):not(:where(.not-prose,.not-prose*)){color:var(--un-prose-captions);font-size:.875em}.prose :where(code):not(:where(.not-prose,.not-prose*)){color:var(--un-prose-code);font-size:.875em;font-weight:600;font-family:var(--un-prose-font-mono)}.prose :where(:not(pre)>code):not(:where(.not-prose,.not-prose*)):after,.prose :where(:not(pre)>code):not(:where(.not-prose,.not-prose*)):before{content:"`"}.prose :where(pre,code):not(:where(.not-prose,.not-prose*)){white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;hyphens:none;background:0 0}.prose :where(ol,ul):not(:where(.not-prose,.not-prose*)){padding-left:1.25em}.prose :where(ul):not(:where(.not-prose,.not-prose*)){list-style-type:disc}.prose :where(ul>li):not(:where(.not-prose,.not-prose*))::marker{color:var(--un-prose-lists)}.prose{color:var(--un-prose-body);max-width:65ch}.prose-lg,[prose-lg=""]{max-width:85ch}[nav-item=""]{display:flex;align-items:center;--un-text-opacity:1;color:rgba(31,41,55,var(--un-text-opacity));opacity:.85}[nav-item=""]>:not([hidden])~:not([hidden]){--un-space-x-reverse:0;margin-left:calc(.25rem * calc(1 - var(--un-space-x-reverse)));margin-right:calc(.25rem * var(--un-space-x-reverse))}.border-c{--un-border-opacity:1;border-color:rgba(229,231,235,var(--un-border-opacity))}.bg-c{--un-bg-opacity:1;background-color:rgba(255,255,255,var(--un-bg-opacity))}[nav-item=""]:hover{--un-text-opacity:1;color:rgba(0,0,0,var(--un-text-opacity))}.text-c{--un-text-opacity:1;color:rgba(31,41,55,var(--un-text-opacity))}.text-c-active{--un-text-opacity:1;color:rgba(23,114,208,var(--un-text-opacity))}[text~=c-lighter]{--un-text-opacity:1;color:rgba(31,41,55,var(--un-text-opacity));opacity:.5}[text~=c-light],[un-text~=c-light]{--un-text-opacity:1;color:rgba(31,41,55,var(--un-text-opacity));opacity:.85}@media (min-width:1024px){.prose-lg,[prose-lg=""]{font-size:1.125rem;line-height:1.75rem}}.absolute{position:absolute}.left-0{left:0}.top-0{top:0}.z-40{z-index:40}.grid{display:grid}.m-auto{margin:auto}[mx-auto=""]{margin-left:auto;margin-right:auto}.mb-6{margin-bottom:1.5rem}[mb-8=""]{margin-bottom:2rem}[mr-1=""]{margin-right:.25rem}[mt-16=""]{margin-top:4rem}.mt-2{margin-top:.5rem}[mt-20=""],[m~=t-20]{margin-top:5rem}[mt-6=""]{margin-top:1.5rem}.inline-block{display:inline-block}.h-14{height:3.5rem}[h-4\.5=""]{height:1.125rem}.min-h-full{min-height:100%}[w-4\.5=""]{width:1.125rem}.w-full{width:100%}.flex,[flex=""]{display:flex}.flex-1{flex:1 1 0%}.flex-col{flex-direction:column}.items-center{align-items:center}.justify-between{justify-content:space-between}[space-x-4=""]>:not([hidden])~:not([hidden]){--un-space-x-reverse:0;margin-left:calc(1rem * calc(1 - var(--un-space-x-reverse)));margin-right:calc(1rem * var(--un-space-x-reverse))}.border-t{border-top-width:1px}[p~=x-4]{padding-left:1rem;padding-right:1rem}.pb-3{padding-bottom:.75rem}.pt-3{padding-top:.75rem}[p~=b-6]{padding-bottom:1.5rem}[p~=t-24]{padding-top:6rem}[text~=center]{text-align:center}.text-left{text-align:left}.text-right{text-align:right}[align-text-top=""]{vertical-align:text-top}.font-sans{font-family:Computer Modern Sans,system-ui,-apple-system,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif}.font-ui,[font~=ui]{font-family:DM Sans}.text-\[0\.95em\]{font-size:.95em}.text-4xl{font-size:2.25rem;line-height:2.5rem}[text-sm=""],[text~=sm]{font-size:.875rem;line-height:1.25rem}[text~=lg]{font-size:1.125rem;line-height:1.75rem}.font-bold{font-weight:700}.leading-12{line-height:3rem}.hover\:underline:hover,[hover\:underline=""]:hover{text-decoration-line:underline}.opacity-50{opacity:.5}@media (max-width:767.9px){.lt-md\:hidden{display:none}}@media (min-width:768px){.md\:grid-cols-2{grid-template-columns:repeat(2,minmax(0,1fr))}[md\:hidden=""]{display:none}}@font-face{font-family:KaTeX_Main;font-style:normal;font-weight:700;src:url(/assets/KaTeX_Main-Bold-0f60d1b8.woff2) format("woff2"),url(/assets/KaTeX_Main-Bold-c76c5d69.woff) format("woff"),url(/assets/KaTeX_Main-Bold-138ac28d.ttf) format("truetype")}@font-face{font-family:KaTeX_Main;font-style:italic;font-weight:700;src:url(/assets/KaTeX_Main-BoldItalic-99cd42a3.woff2) format("woff2"),url(/assets/KaTeX_Main-BoldItalic-a6f7ec0d.woff) format("woff"),url(/assets/KaTeX_Main-BoldItalic-70ee1f64.ttf) format("truetype")}@font-face{font-family:KaTeX_Main;font-style:italic;font-weight:400;src:url(/assets/KaTeX_Main-Italic-97479ca6.woff2) format("woff2"),url(/assets/KaTeX_Main-Italic-f1d6ef86.woff) format("woff"),url(/assets/KaTeX_Main-Italic-0d85ae7c.ttf) format("truetype")}@font-face{font-family:KaTeX_Main;font-style:normal;font-weight:400;src:url(/assets/KaTeX_Main-Regular-c2342cd8.woff2) format("woff2"),url(/assets/KaTeX_Main-Regular-c6368d87.woff) format("woff"),url(/assets/KaTeX_Main-Regular-d0332f52.ttf) format("truetype")}@font-face{font-family:KaTeX_Math;font-style:italic;font-weight:700;src:url(/assets/KaTeX_Math-BoldItalic-dc47344d.woff2) format("woff2"),url(/assets/KaTeX_Math-BoldItalic-850c0af5.woff) format("woff"),url(/assets/KaTeX_Math-BoldItalic-f9377ab0.ttf) format("truetype")}@font-face{font-family:KaTeX_Math;font-style:italic;font-weight:400;src:url(/assets/KaTeX_Math-Italic-7af58c5e.woff2) format("woff2"),url(/assets/KaTeX_Math-Italic-8a8d2445.woff) format("woff"),url(/assets/KaTeX_Math-Italic-08ce98e5.ttf) format("truetype")}.katex{text-rendering:auto;font:1.21em KaTeX_Main,Times New Roman,serif;line-height:1.2;text-indent:0}.katex *{-ms-high-contrast-adjust:none!important;border-color:currentColor}.katex .base{position:relative;white-space:nowrap;width:-webkit-min-content;width:-moz-min-content;width:min-content}.katex .base,.katex .strut{display:inline-block}.katex .mathnormal{font-family:KaTeX_Math;font-style:italic}.katex .vlist-t{border-collapse:collapse;display:inline-table;table-layout:fixed}.katex .vlist-r{display:table-row}.katex .vlist{display:table-cell;position:relative;vertical-align:bottom}.katex .vlist>span{display:block;height:0;position:relative}.katex .vlist>span>span{display:inline-block}.katex .vlist>span>.pstrut{overflow:hidden;width:0}.katex .vlist-t2{margin-right:-2px}.katex .vlist-s{display:table-cell;font-size:1px;min-width:2px;vertical-align:bottom;width:2px}.katex .msupsub{text-align:left}.katex .mfrac>span>span{text-align:center}.katex .mfrac .frac-line{border-bottom-style:solid;display:inline-block;width:100%}.katex .mfrac .frac-line{min-height:1px}.katex .mspace{display:inline-block}.katex .sizing.reset-size3.size1{font-size:.71428571em}.katex .sizing.reset-size3.size6{font-size:1.42857143em}.katex .sizing.reset-size6.size3{font-size:.7em}.katex .nulldelimiter{display:inline-block;width:.12em}.katex .svg-align{text-align:left}.katex svg{fill:currentColor;stroke:currentColor;fill-rule:nonzero;fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1;display:block;height:inherit;position:absolute;width:100%}.katex svg path{stroke:none}.katex .hide-tail{overflow:hidden;position:relative;width:100%}.katex-display{display:block;margin:1em 0;text-align:center}.katex-display>.katex{display:block;text-align:center;white-space:nowrap}.katex-display>.katex>.katex-html{display:block;position:relative}body{counter-reset:katexEqnNo mmlEqnNo}.prose p{line-height:1.55!important}.prose li p,.prose ul{margin:0!important}.prose a{font-weight:400;--un-text-opacity:1;color:rgba(23,114,208,var(--un-text-opacity));text-decoration:none}.prose a:hover{text-decoration-line:underline}.prose img{margin-left:auto;margin-right:auto;margin-top:.5rem;margin-bottom:.5rem}.prose figure figcaption{font-size:.875rem;line-height:1.25rem;--un-text-opacity:1;color:rgba(31,41,55,var(--un-text-opacity));opacity:.85}.prose .katex-display{overflow-x:auto;overflow-y:hidden;-webkit-overflow-scrolling:touch}.prose .katex{font-size:1.1em}.post a.header-anchor{float:left;--un-text-opacity:1;color:rgba(31,41,55,var(--un-text-opacity));text-decoration:none;opacity:0;margin-left:-1.3em;font-size:.75em}.post h2:focus .header-anchor,.post h2:hover .header-anchor,.post h3:focus .header-anchor,.post h3:hover .header-anchor{opacity:.5}@media (max-width:640px){.post a.header-anchor{display:none}}.table-of-contents{position:fixed;top:5rem;right:0;bottom:0;z-index:30;display:none;width:13.75rem;overflow-y:auto;overflow-x:hidden;text-overflow:ellipsis;padding-top:4.5rem;padding-right:1.375rem;text-align:right;font-size:.8em}.toc-open .table-of-contents{display:block}.table-of-contents ul{margin:0!important;list-style-type:none!important}.table-of-contents ul>li>a{--un-text-opacity:1;color:rgba(31,41,55,var(--un-text-opacity))}.table-of-contents>ul>li:before{margin-right:.25rem;opacity:.6;content:"#"}.table-of-contents>ul>li>ul>li{opacity:.6}.table-of-contents::-webkit-scrollbar{display:none}@media (max-width:1200px){.table-of-contents{top:0;border-left-width:1px;--un-border-opacity:1;border-color:rgba(229,231,235,var(--un-border-opacity));--un-bg-opacity:1;background-color:rgba(255,255,255,var(--un-bg-opacity));--un-shadow:var(--un-shadow-inset) 0 1px 3px 0 var(--un-shadow-color, rgba(0,0,0,.1)),var(--un-shadow-inset) 0 1px 2px -1px var(--un-shadow-color, rgba(0,0,0,.1));box-shadow:var(--un-ring-offset-shadow),var(--un-ring-shadow),var(--un-shadow)}}@font-face{font-family:Computer Modern Sans;font-style:normal;font-weight:400;src:url(/assets/regular-5b933bfd.woff) format("woff")}@font-face{font-family:Computer Modern Sans;font-style:normal;font-weight:700;src:url(/assets/bold-48de7558.woff) format("woff")}@font-face{font-family:Computer Modern Sans;font-style:italic;font-weight:400;src:url(/assets/italic-c0fd060e.woff) format("woff")}@font-face{font-family:Computer Modern Sans;font-style:italic;font-weight:700;src:url(/assets/bolditalic-28145ed6.woff) format("woff")}#app,body,html{margin:0;height:100%;padding:0}.prev a[data-v-9b596018]:before{content:"\2190  "}.next a[data-v-9b596018]:after{content:" \2192"}</style><link rel="preload" href="/assets/index-e6203160.css" as="style"><link rel="modulepreload" crossorigin="" href="/assets/post-2c194bb0.js"><link rel="preload" href="/assets/post-1d6a6dca.css" as="style"><link rel="modulepreload" crossorigin="" href="/assets/2020-07-17-transformer-a2959e88.js"><title>试图理一理 Transformer</title><meta name="description" content="A dragon lost in human world."><meta name="theme-color" content="#ffffff"><meta property="og:title" content="试图理一理 Transformer"><link rel="preload" as="font" crossorigin="anonymous" href="https://fonts.gstatic.com/s/dmsans/v11/rP2Hp2ywxg089UriCZ2IHSeH.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="https://fonts.gstatic.com/s/dmsans/v11/rP2Hp2ywxg089UriCZOIHQ.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="https://fonts.gstatic.com/s/dmsans/v11/rP2Cp2ywxg089UriASitCBamC2QX.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="https://fonts.gstatic.com/s/dmsans/v11/rP2Cp2ywxg089UriASitCBimCw.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_AMS-Regular-0cdd387c.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Caligraphic-Bold-de7701e4.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Caligraphic-Regular-5d53e70a.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Fraktur-Bold-74444efd.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Fraktur-Regular-51814d27.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Main-Bold-0f60d1b8.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Main-BoldItalic-99cd42a3.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Main-Italic-97479ca6.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Main-Regular-c2342cd8.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Math-BoldItalic-dc47344d.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Math-Italic-7af58c5e.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_SansSerif-Bold-e99ae511.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_SansSerif-Italic-00b26ac8.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_SansSerif-Regular-68e8c73e.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Script-Regular-036d4e95.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Size1-Regular-6b47c401.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Size2-Regular-d04c5421.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="data:font/woff2;base64,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"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Size4-Regular-a4af7d41.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Typewriter-Regular-71d517d6.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/regular-5b933bfd.woff"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/bold-48de7558.woff"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/italic-c0fd060e.woff"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/bolditalic-28145ed6.woff"></head><body class="font-sans"><div id="app" data-server-rendered="true"><main class="flex flex-col min-h-full text-c post" p="x-4 t-24 b-6" data-v-9b596018=""><header class="z-40 w-full h-14 flex justify-between items-center bg-c font-ui absolute top-0 left-0" p="x-4 md:x-5" data-v-4d808a77=""><a href="/" class="font-bold" un-text="c-light hover:c-dark" data-v-4d808a77=""><span text="lg" data-v-4d808a77="">hi@zxh</span><div i-fa6-solid:angle-right="" class="prompt inline-block" data-v-4d808a77=""></div><span class="blink" data-v-4d808a77="">_</span></a><nav flex="" space-x-4="" data-v-4d808a77=""><a href="/projects" class="" nav-item="" title="Projects" data-v-4d808a77=""><div i-ph:rocket-launch-duotone="" md:hidden="" data-v-4d808a77=""></div><span class="lt-md:hidden" data-v-4d808a77="">Projects</span></a><a href="/posts" class="" nav-item="" title="Blog" data-v-4d808a77=""><div i-majesticons:pencil-line="" md:hidden="" data-v-4d808a77=""></div><span class="lt-md:hidden" data-v-4d808a77="">Blog</span></a><a nav-item="" href="/feed.xml" title="RSS" target="_blank" rel="noopener noreferrer" data-v-4d808a77=""><div i-jam:rss-feed="" data-v-4d808a77=""></div></a><button nav-item="" title="Toggle dark" data-v-4d808a77=""><div i="carbon-sun dark:carbon-moon" data-v-4d808a77=""></div></button><!--[--><!--]--></nav></header><div class="flex-1 mb-6"><!--[--><div prose-lg="" mx-auto="" mt-6="" mb-8="" data-v-9b596018=""><h1 class="text-4xl font-bold leading-12" data-v-9b596018="">试图理一理 Transformer</h1><p class="opacity-50 mt-2" data-v-9b596018="">Jul 17, 2020 · 9 min <span data-v-9b596018="">· <span i-uil:tag-alt="" mr-1="" text-sm="" data-v-9b596018=""></span><!--[--><span data-v-9b596018=""><a href="/posts/zh/tags/deep-learning" class="" hover:underline="" data-v-9b596018="">deep learning</a><!----></span><!--]--></span></p></div><article class="toc-open" data-v-9b596018=""><div class="prose prose-lg m-auto text-left" data-v-9b596018=""><p></p><div class="table-of-contents"><ul><li><a href="#position-embedding">Position Embedding</a></li><li><a href="#encoder">Encoder</a><ul><li><a href="#muti-head-self-attention">Muti-Head Self-Attention</a></li><li><a href="#feed-forward-network">Feed-Forward Network</a></li></ul></li><li><a href="#decoder">Decoder</a><ul><li><a href="#masked-multi-head-self-attention">Masked Multi-Head Self-Attention</a></li><li><a href="#multi-head-attention">Multi-head Attention</a></li><li><a href="#feed-forward-network-1">Feed-Forward Network</a></li></ul></li><li><a href="#summary">Summary</a></li><li><a href="#reference">Reference</a></li></ul></div><p></p><p><strong>Attention Is All You Need.</strong> <em>Ashish Vaswani, et al.</em> NIPS 2017. <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener noreferrer">[Paper]</a> <a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py" target="_blank" rel="noopener noreferrer">[Code]</a></p><p>考虑到 RNN 只能单向依次计算，所以存在以下问题：</p><ul><li><p><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6151em"></span><span class="mord mathnormal">t</span></span></span></span> 时刻的计算依赖与 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6984em;vertical-align:-.0833em"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord">1</span></span></span></span> 时刻的计算结果，限制了模型的并行能力</p></li><li><p>RNN 的长期依赖问题</p></li></ul><p>于是这篇论文扔掉了 encoder 和 decoder 中的 RNN 结构，完全用 attention 来搞 machine translation：</p><ul><li><p>没有 RNN 结构，所以有更好的并行能力</p></li><li><p>attention 机制对全局信息的处理更有效</p></li></ul><p>Transformer 整体结构如下：</p><p><img src="/img/posts/zh/2020-07-17/transformer.png" alt="Transformer" width="400"></p><h2 id="position-embedding" tabindex="-1">Position Embedding <a class="header-anchor" href="#position-embedding" aria-hidden="true">#</a></h2><p>Transformer 扔掉了 RNN，对输入句子的所有单词都是同时处理的，所以失去了捕捉单词的排序和位置信息的能力。如果不解决词序的问题，那即使把一句话打乱，attention 出来的结果也是一样的，相当于这就只是一个词袋模型。为了解决这个问题，论文引入 position embedding 来对单词的位置信息进行编码。最终的输入词向量 = word embedding + position embedding：</p><p></p><figure alt="Positional Embedding"><img src="/img/posts/zh/2020-07-17/positional-embedding.png" alt="Positional Embedding"><figcaption>图片来源：<a href="http://jalammar.github.io/illustrated-transformer#representing-the-order-of-the-sequence-using-positional-encoding" target="_blank" rel="noopener noreferrer">The Illustrated Transformer</a></figcaption></figure><p></p><p>有两种搞到 position embedding 的思路：</p><ul><li><p>学习出一份 position embedding（<a href="http://proceedings.mlr.press/v70/gehring17a/gehring17a.pdf" target="_blank" rel="noopener noreferrer"><strong>Convolutional Sequence to Sequence Learning</strong></a>. <em>Jonas Gehring et al.</em> ICML 2017.）</p></li><li><p>直接用不同频率的 sin 和 cos 函数算出来</p></li></ul><p>经过实验，论文发现这俩方法效果差不多，所以选了第二种方法，因为它有以下好处：</p><ul><li><p>不需要加额外的训练参数</p></li><li><p>学习出来的 position embedding 会受到训练集中序列的长度的限制，但三角函数明显不受序列长度的限制，所以能够处理训练集中没见过的序列长度</p></li></ul><p>具体的位置编码公式为：</p><p><span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.05764em">PE</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord">2</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:2.1195em;vertical-align:-1.0119em"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1076em"><span style="top:-2.11em"><span class="pstrut" style="height:3.1219em"></span><span class="mord"><span class="mord">1000</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1219em"><span style="top:-3.5234em;margin-right:.05em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8551em"><span style="top:-2.656em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3448em"><span style="top:-2.3448em;margin-left:0;margin-right:.1em"><span class="pstrut" style="height:2.6944em"></span><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.3496em"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.2255em"><span class="pstrut" style="height:3em"></span><span class="frac-line mtight" style="border-bottom-width:.049em"></span></span><span style="top:-3.384em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.5937em"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.3519em"><span class="pstrut" style="height:3.1219em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.7989em"><span class="pstrut" style="height:3.1219em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0119em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.05764em">PE</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord">2</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:2.1195em;vertical-align:-1.0119em"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1076em"><span style="top:-2.11em"><span class="pstrut" style="height:3.1219em"></span><span class="mord"><span class="mord">1000</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1219em"><span style="top:-3.5234em;margin-right:.05em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8551em"><span style="top:-2.656em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3448em"><span style="top:-2.3448em;margin-left:0;margin-right:.1em"><span class="pstrut" style="height:2.6944em"></span><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.3496em"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.2255em"><span class="pstrut" style="height:3em"></span><span class="frac-line mtight" style="border-bottom-width:.049em"></span></span><span style="top:-3.384em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.5937em"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.3519em"><span class="pstrut" style="height:3.1219em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.7989em"><span class="pstrut" style="height:3.1219em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0119em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span></p><p>其中 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> 为词嵌入维度（论文中为 512），pos 为该单词在序列中的位置，<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6595em"></span><span class="mord">2</span><span class="mord mathnormal">i</span></span></span></span> 为词向量的偶数维度（用第一个公式），<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7429em;vertical-align:-.0833em"></span><span class="mord">2</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord">1</span></span></span></span> 指词向量的奇数维度（用第二个公式）。波的频率和偏移对于每个维度是不同的：</p><p></p><figure alt="wave"><img src="/img/posts/zh/2020-07-17/wave.png" alt="wave"><figcaption>图片来源：<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding" target="_blank" rel="noopener noreferrer">The Annotated Transformer</a></figcaption></figure><p></p><p>因为三角函数还有以下特性：</p><p><span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.0037em">α</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.05278em">β</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.0037em">α</span><span class="mclose">)</span><span class="mspace" style="margin-right:.1667em"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.05278em">β</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.0037em">α</span><span class="mclose">)</span><span class="mspace" style="margin-right:.1667em"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.05278em">β</span><span class="mclose">)</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.0037em">α</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.05278em">β</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.0037em">α</span><span class="mclose">)</span><span class="mspace" style="margin-right:.1667em"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.05278em">β</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.0037em">α</span><span class="mclose">)</span><span class="mspace" style="margin-right:.1667em"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.05278em">β</span><span class="mclose">)</span></span></span></span></span></p><p>所以任意位置的 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.05764em">PE</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span><span class="mclose">)</span></span></span></span> 都能通过 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.05764em">PE</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mclose">)</span></span></span></span> 线性表达，这为模型捕捉单词之间的相对位置关系提供了非常大的便利。</p><h2 id="encoder" tabindex="-1">Encoder <a class="header-anchor" href="#encoder" aria-hidden="true">#</a></h2><p>论文中的 encoder 由 N = 6 个相同的 layer 堆叠而成：</p><p><img src="/img/posts/zh/2020-07-17/encoder.png" alt="encoder" width="180"></p><p>每个 layer 由两个 sub-layer 组成，分别为 multi-head self-attention 和 fully connected feed-forward network。</p><p>并且每个 sub-layer 都加了：</p><ul><li><p>Residual Connection：解决多层神经网络训练困难的问题，通过将前一层的信息无差的传递到下一层，可以有效的仅关注差异部分</p></li><li><p>Layer Normalisation：对层的激活值进行归一化，可以加速模型的训练过程，使其更快的收敛</p><p><a href="https://arxiv.org/pdf/1607.06450.pdf" target="_blank" rel="noopener noreferrer"><strong>Layer Normalization</strong></a>. <em>Jimmy Lei Ba, et al.</em> arXiv 2016.</p></li></ul><p>也就是输入会先进 LayerNorm，再进 sub-layer，然后加在原始输入上（虽然图上 LayerNorm 似乎在 sub-layer 后面，但<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html#encoder" target="_blank" rel="noopener noreferrer">代码</a>里的确是先进 LayerNorm）。最后 6 个 layer 都跑完之后还要再单独 norm 一次（虽然图上没画但<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html#encoder" target="_blank" rel="noopener noreferrer">代码</a>里写了）。</p><h3 id="muti-head-self-attention" tabindex="-1">Muti-Head Self-Attention <a class="header-anchor" href="#muti-head-self-attention" aria-hidden="true">#</a></h3><p>attention 可以表示为以下形式：</p><p><span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.9251em;vertical-align:-.31em"></span><span class="mord mathnormal">a</span><span class="mord mathnormal">tt</span><span class="mord" style="margin-right:.02778em">_</span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span><span class="mclose">)</span></span></span></span></span></p><p>其中 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span></span></span></span>（value）用来求加权和得到最终的上下文向量，而 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8778em;vertical-align:-.1944em"></span><span class="mord mathnormal">Q</span></span></span></span>（query）和所有的 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span></span></span></span>（key）会被用来计算注意力权重。如在传统的 seq2seq 结构中，它们分别由以下值经过线性变换得到：</p><ul><li><p><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8778em;vertical-align:-.1944em"></span><span class="mord mathnormal">Q</span></span></span></span>：decoder 的当前输入</p></li><li><p><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span></span></span></span>：encoder 的输出（<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3011em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3011em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>）</p></li><li><p><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span></span></span></span>：同 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span></span></span></span></p></li></ul><p>而这里是 self-attention，所以 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8778em;vertical-align:-.1944em"></span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span></span></span></span> 由同一个值 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">x</span></span></span></span> 经过线性变换得到，<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">x</span></span></span></span> 在第一个 layer 为输入的词向量序列，在之后的 layer 则为上一个 layer 的输出。</p><p>而 multi-head attention 就是通过 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6944em"></span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord">8</span></span></span></span> 个不同的线性变换得到不同的 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8778em;vertical-align:-.1944em"></span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span></span></span></span>，最后将这 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6944em"></span><span class="mord mathnormal">h</span></span></span></span> 个 attention 结果拼接起来：</p><p></p><figure alt="multi-head self-attention"><img src="/img/posts/zh/2020-07-17/multi-head-self-attention.png" alt="multi-head self-attention"><figcaption>图片来源：<a href="http://jalammar.github.io/illustrated-transformer#the-beast-with-many-heads" target="_blank" rel="noopener noreferrer">The Illustrated Transformer</a></figcaption></figure><p></p><p>这里的 attention 计算公式为（scaled dot-product）：</p><p><span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:2.4483em;vertical-align:-.93em"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em"><span style="top:-2.2528em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8572em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:.833em"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:.853em;height:1.08em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.1828em"><span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:.07153em">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.93em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:.22222em">V</span></span></span></span></span></p><p>注意：这里跟 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span></span></span></span> 是<strong>矩阵相乘</strong>，不是 element-wise 相乘。</p><p><img src="/img/posts/zh/2020-07-17/attention.png" alt="attention"></p><p>其中 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">512/8</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord">64</span></span></span></span>。除以 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-.1828em"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8572em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:.833em"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:.853em;height:1.08em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.1828em"><span></span></span></span></span></span></span></span></span> 是因为，<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> 越大 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0358em;vertical-align:-.1944em"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:.07153em">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span></span></span></span> 就会越大，可能就会将 softmax 函数推入梯度极小的区域，所以要用 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-.1828em"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8572em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:.833em"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:.853em;height:1.08em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.1828em"><span></span></span></span></span></span></span></span></span> 对 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0358em;vertical-align:-.1944em"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:.07153em">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span></span></span></span> 进行缩放。</p><p>图中的 mask 只会在 <a href="#masked-multi-head-self-attention">decoder</a> 中被用到。</p><h3 id="feed-forward-network" tabindex="-1">Feed-Forward Network <a class="header-anchor" href="#feed-forward-network" aria-hidden="true">#</a></h3><p>第二个 sub-layer 是一个前馈网络：</p><p><span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord text"><span class="mord">FFN</span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal">x</span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3011em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3011em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3011em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.8444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3011em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></span></p><h2 id="decoder" tabindex="-1">Decoder <a class="header-anchor" href="#decoder" aria-hidden="true">#</a></h2><p>encoder-decoder 结构：</p><p></p><figure alt="encoder-decoder"><img src="/img/posts/zh/2020-07-17/encoder-decoder.png" alt="encoder-decoder"><figcaption>图片来源：<a href="http://jalammar.github.io/illustrated-transformer#the-residuals" target="_blank" rel="noopener noreferrer">The Illustrated Transformer</a></figcaption></figure><p></p><p><a href="http://jalammar.github.io/illustrated-transformer#the-decoder-side" target="_blank" rel="noopener noreferrer">这里</a>还有两个清楚的解释了 encoder 和 decoder 的工作方式的动画。</p><p>decoder 也由 N = 6 个相同的 layer 堆叠而成，每个 layer 由三个 sub-layer 组成：</p><p><img src="/img/posts/zh/2020-07-17/decoder.png" alt="decoder" width="180"></p><h3 id="masked-multi-head-self-attention" tabindex="-1">Masked Multi-Head Self-Attention <a class="header-anchor" href="#masked-multi-head-self-attention" aria-hidden="true">#</a></h3><p>在训练时，decoder 在预测第 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6595em"></span><span class="mord mathnormal">i</span></span></span></span> 个位置时不应该看到未来的信息，但 self-attention 机制能让它看到全局信息（标签泄露）。所以会对在 self-attention 的 softmax 层前加 mask，将未来信息屏蔽掉。</p><p>mask 是一个下三角矩阵，对角线以及对角线左下都是1，其余都是0：</p><p></p><figure alt="mask"><img src="/img/posts/zh/2020-07-17/mask.png" alt="mask" width="300"><figcaption>mask 矩阵，蓝色部分是 1，白色部分是 0（图片来源：<a href="https://spaces.ac.cn/archives/6933#%E5%8D%95%E5%90%91%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B" target="_blank" rel="noopener noreferrer">从语言模型到 Seq2Seq：Transformer 如戏，全靠 Mask</a>）</figcaption></figure><p></p><p>矩阵的行为当前预测到第几个单词，列为当前允许看到前几个位置的信息。然后 mask=0 的位置上的元素会都被替换为 <code>-inf</code>。</p><h3 id="multi-head-attention" tabindex="-1">Multi-head Attention <a class="header-anchor" href="#multi-head-attention" aria-hidden="true">#</a></h3><p>即论文 3.2.3 节中的 encoder-decoder attention。它的 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8778em;vertical-align:-.1944em"></span><span class="mord mathnormal">Q</span></span></span></span> 来自于上一位置的 decoder 的输出（第一个 layer）或上一个 decoder layer 的输出（之后的 layer），而 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span></span></span></span> 和 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span></span></span></span> 来自于 encoder 的输出。这让 decoder 的每一个位置都可以看到到输入序列的全局信息。</p><p>编码可以并行计算，但解码时，因为需要上一时刻的输出当作 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8778em;vertical-align:-.1944em"></span><span class="mord mathnormal">Q</span></span></span></span>，所以无法并行计算。</p><h3 id="feed-forward-network-1" tabindex="-1">Feed-Forward Network <a class="header-anchor" href="#feed-forward-network-1" aria-hidden="true">#</a></h3><p>同 encoder。</p><h2 id="summary" tabindex="-1">Summary <a class="header-anchor" href="#summary" aria-hidden="true">#</a></h2><p>优点：</p><ul><li><p>相比其他方法，当序列长度 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">n</span></span></span></span> 小于词向量维度 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6944em"></span><span class="mord mathnormal">d</span></span></span></span> 时，每层的计算复杂度（complexity per layer）更低：</p><p><img src="/img/posts/zh/2020-07-17/complexity.png" alt="complexity"></p></li><li><p>更好的并行性，符合目前的硬件（GPU）环境</p></li><li><p>更好地处理长时依赖问题：如果要处理一个长度为 n 的序列，CNN 需要增加卷积层数来扩大视野，RNN 需要从 1 到 n 逐个进行计算，而 self-attention 只需要一步矩阵运算就可以</p></li></ul><p>缺点：</p><ul><li><p>但同时从上面那张复杂度表里也能看出来，当句子太长时，Transformer <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 的时间复杂度是非常爆炸的。Transformer 能更好地处理长时依赖问题，但这种复杂度又让它没法处理太长的文本，即使是 Bert 的最大长度也只有 512。</p><p>于是出现了一堆致力于解决这个问题的后续工作，等我摸两天鱼再看看有没有空写这个…</p></li><li><p>扔掉了 RNN 和 CNN，导致失去了捕捉局部特征的能力</p><p>不过论文也提到了一个 restricted self-attention（上面那张复杂度表里有），它假设当前词只与前后 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal" style="margin-right:.02778em">r</span></span></span></span> 个词有关，因此只在这 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7278em;vertical-align:-.0833em"></span><span class="mord">2</span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord">1</span></span></span></span> 个词上做 attention，复杂度是 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mclose">)</span></span></span></span>，相当于是在捕捉局部特征。听上去很像卷积窗口？</p></li><li><p>失去的位置信息非常重要，在词向量中加入 position embedding 这个解决方案依然不够好</p></li><li><p>非图灵完备（computationally universal）</p><p><a href="https://openreview.net/pdf?id=HyzdRiR9Y7" target="_blank" rel="noopener noreferrer"><strong>Universal Transformer</strong></a>. <em>Mostafa Dehghani, et al.</em> ICLR 2019.</p></li></ul><h2 id="reference" tabindex="-1">Reference <a class="header-anchor" href="#reference" aria-hidden="true">#</a></h2><ul><li><p>图解 Transformer：<a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer">The Illustrated Transformer</a></p></li><li><p>连着代码一起讲：<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener noreferrer">The Annotated Transformer</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/44121378" target="_blank" rel="noopener noreferrer">【NLP】Transformer 详解</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/48508221" target="_blank" rel="noopener noreferrer">详解 Transformer（Attention Is All You Need）</a></p></li></ul></div></article><div prose-lg="" mx-auto="" mt-16="" data-v-9b596018=""><div class="grid md:grid-cols-2 pb-3 text-[0.95em]" data-v-9b596018=""><a href="https://github.com/Renovamen/renovamen.github.io/edit/main/pages/posts/zh/2020-07-17-transformer.md" title="Edit link" target="_blank" rel="noopener noreferrer" class="hover:underline text-c-active" data-v-9b596018=""><span i-tabler:edit="" w-4.5="" h-4.5="" align-text-top="" data-v-9b596018=""></span> Edit this page on GitHub</a><span text="md:right c-light" data-v-9b596018=""><span i-ic:round-update="" w-4.5="" h-4.5="" data-v-9b596018=""></span> Last updated: 2/6/2023, 11:57:43 AM</span></div><div class="grid md:grid-cols-2 pt-3 text-[0.95em] border-t border-c" data-v-9b596018=""><span class="prev" data-v-9b596018=""><a href="/posts/zh/2020-07-10-messy-notes" class="" hover:underline="" data-v-9b596018="">乱七八糟的知识点</a></span><span class="next text-right" data-v-9b596018=""><a href="/posts/zh/2020-08-05-meta-learning" class="" hover:underline="" data-v-9b596018="">Meta Learning：一种套娃算法</a></span></div><div class="giscus" mt-20="" data-v-9b596018=""></div></div><!--]--></div><footer font="ui" text="sm center c-lighter" m="t-20">© Xiaohan Zou 2023<br>A dragon lost in human world</footer></main></div><script>window.__INITIAL_STATE__='{"pinia":{}}'</script><link rel="stylesheet" href="/assets/index-e6203160.css"><link rel="stylesheet" href="/assets/post-1d6a6dca.css"></body></html>